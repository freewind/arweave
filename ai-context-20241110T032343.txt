你是一个资深的程序员，对于各种语言、框架、和最佳实践都有非常丰富的经验，擅长从样例代码中学习风格和模式。帮我完成以下编程相关的任务，谢谢。

<任务文件>
  - /Users/peng.li/workspace/arweave/apps/arweave/src/ar_base32.erl
  - /Users/peng.li/workspace/arweave/apps/arweave/src/ar_base32.erl
  - /Users/peng.li/workspace/arweave/apps/arweave/src/ar_base32.erl
  - /Users/peng.li/workspace/arweave/apps/arweave/src/ar_base32.erl
  - /Users/peng.li/workspace/arweave/apps/arweave/src/ar_bench_hash.erl
  - /Users/peng.li/workspace/arweave/apps/arweave/src/ar_bench_packing.erl
  - /Users/peng.li/workspace/arweave/apps/arweave/src/ar_bench_timer.erl
  - /Users/peng.li/workspace/arweave/apps/arweave/src/ar_bench_vdf.erl
  - /Users/peng.li/workspace/arweave/apps/arweave/src/ar_blacklist_middleware.erl
  - /Users/peng.li/workspace/arweave/apps/arweave/src/ar_block_cache.erl
  - /Users/peng.li/workspace/arweave/apps/arweave/src/ar_block_index.erl
</任务文件>

<AI请注意>
  - 禁止迎合我，你必须坚持自己的专业判断
  - 必须显示完整文件路径，这是强制要求
  - 严禁分步骤完成任务，除非我明确要求
  - 严禁提问和确认，直接完成任务
  - 严禁废话，只说必要的内容
  - 每次必须按顺序处理所有相关文件，不允许遗漏
</AI请注意>

<任务目标>
    我是一个对erlang语言完全不了解的新人，并且英文也不好。想通过这个项目学习这门语言，并且学习项目的逻辑和原理。
    请你为每个文件erlang文件加入详细的中文注释，要详细到让一个傻瓜也能看懂。内容包括但不限于
    1. 各种语法的解释（所有erlang语法）
    2. 不常见单词的中文翻译。比如 `monotonic (中文解释）`
    3. 用到的内置库和第三方组件的解释（有什么用，有什么历史，有什么类似的库）
    4. 逻辑解释。比如数学、加密逻辑、通信协议等等。
    5. 具体到每行代码的解释
    6. 如果代码内有英文注释，要在其旁边写入相应的中文翻译，但不要删除原有的英文，以便我对比
	7. 在每个文件的最上方，用长的 ====== 标记出一大块注释，是对整个文件的目的、作用、主要逻辑、关键点进行描述，方便我快速了解当前文档内容
	
    始终记得是在为一个完全不懂erlang并且英文不好的人在写注释，方便他学习
    绝对禁止偷懒，绝对禁止先干一点就确认，必须一次性完全所有工作。如果有问题，我查看后会再提出修改建议
</任务目标>

<file path="/Users/peng.li/workspace/arweave/apps/arweave/src/ar_base32.erl">
<![CDATA[
%% @doc This module is very strongly inspired by OTP's base64 source code.
%% See https://github.com/erlang/otp/blob/93ec8bb2dbba9456395a54551fe9f1e0f86184b1/lib/stdlib/src/base64.erl#L66-L80
-module(ar_base32).

-export([encode/1]).

%%%===================================================================
%%% Public interface.
%%%===================================================================

%% @doc Encode data into a lowercase unpadded RFC 4648 base32 alphabet
encode(Bin) when is_binary(Bin) ->
	encode_binary(Bin, <<>>).

%%%===================================================================
%%% Private functions.
%%%===================================================================

encode_binary(<<>>, A) ->
	A;
encode_binary(<<B1:8>>, A) ->
	<<A/bits, (b32e(B1 bsr 3)):8, (b32e((B1 band 7) bsl 2)):8>>;
encode_binary(<<B1:8, B2:8>>, A) ->
	BB = (B1 bsl 8) bor B2,
	<<A/bits,
		(b32e(BB bsr 11)):8,
		(b32e((BB bsr 6) band 31)):8,
		(b32e((BB bsr 1) band 31)):8,
		(b32e((BB bsl 4) band 31)):8>>;
encode_binary(<<B1:8, B2:8, B3:8>>, A) ->
	BB = (B1 bsl 16) bor (B2 bsl 8) bor B3,
	<<A/bits,
		(b32e(BB bsr 19)):8,
		(b32e((BB bsr 14) band 31)):8,
		(b32e((BB bsr 9) band 31)):8,
		(b32e((BB bsr 4) band 31)):8,
		(b32e((BB bsl 1) band 31)):8>>;
encode_binary(<<B1:8, B2:8, B3:8, B4:8>>, A) ->
	BB = (B1 bsl 24) bor (B2 bsl 16) bor (B3 bsl 8) bor B4,
	<<A/bits,
		(b32e(BB bsr 27)):8,
		(b32e((BB bsr 22) band 31)):8,
		(b32e((BB bsr 17) band 31)):8,
		(b32e((BB bsr 12) band 31)):8,
		(b32e((BB bsr 7) band 31)):8,
		(b32e((BB bsr 2) band 31)):8,
		(b32e((BB bsl 3) band 31)):8>>;
encode_binary(<<B1:8, B2:8, B3:8, B4:8, B5:8, Ls/bits>>, A) ->
	BB = (B1 bsl 32) bor (B2 bsl 24) bor (B3 bsl 16) bor (B4 bsl 8) bor B5,
	encode_binary(
		Ls,
		<<A/bits,
			(b32e(BB bsr 35)):8,
			(b32e((BB bsr 30) band 31)):8,
			(b32e((BB bsr 25) band 31)):8,
			(b32e((BB bsr 20) band 31)):8,
			(b32e((BB bsr 15) band 31)):8,
			(b32e((BB bsr 10) band 31)):8,
			(b32e((BB bsr 5) band 31)):8,
			(b32e(BB band 31)):8>>
	).

-compile({inline, [{b32e, 1}]}).
b32e(X) ->
	element(X+1, {
		$a, $b, $c, $d, $e, $f, $g, $h, $i, $j, $k, $l, $m,
		$n, $o, $p, $q, $r, $s, $t, $u, $v, $w, $x, $y, $z,
		$2, $3, $4, $5, $6, $7, $8, $9
	}).

]]>
</file>

<file path="/Users/peng.li/workspace/arweave/apps/arweave/src/ar_bench_hash.erl">
<![CDATA[
-module(ar_bench_hash).

-export([run_benchmark_from_cli/1, run_benchmark/1]).

-include_lib("arweave/include/ar_consensus.hrl").
-include_lib("arweave/include/ar_config.hrl").

run_benchmark_from_cli(Args) ->
	RandomX = get_flag_value(Args, "randomx", "512"),
	JIT = list_to_integer(get_flag_value(Args, "jit", "1")),
	LargePages = list_to_integer(get_flag_value(Args, "large_pages", "1")),
	HardwareAES = list_to_integer(get_flag_value(Args, "hw_aes", "1")),

	RandomXMode = case RandomX of
		"512" -> rx512;
		"4096" -> rx4096;
		_ -> show_help()
	end,

	Schedulers = erlang:system_info(dirty_cpu_schedulers_online),
	RandomXState = ar_mine_randomx:init_fast2(
		RandomXMode, ?RANDOMX_PACKING_KEY, JIT, LargePages, Schedulers),
	{H0, H1} = run_benchmark(RandomXState, JIT, LargePages, HardwareAES),
	H0String = io_lib:format("~.3f", [H0 / 1000]),
	H1String = io_lib:format("~.3f", [H1 / 1000]),
	ar:console("Hashing benchmark~nH0: ~s ms~nH1/H2: ~s ms~n", [H0String, H1String]).

get_flag_value([], _, DefaultValue) ->
    DefaultValue;
get_flag_value([Flag | [Value | _Tail]], TargetFlag, _DefaultValue) when Flag == TargetFlag ->
    Value;
get_flag_value([_ | Tail], TargetFlag, DefaultValue) ->
    get_flag_value(Tail, TargetFlag, DefaultValue).

show_help() ->
	io:format("~nUsage: benchmark-hash [options]~n"),
	io:format("Options:~n"),
	io:format("  randomx <512|4096> (default: 512)~n"),
	io:format("  jit <0|1> (default: 1)~n"),
	io:format("  large_pages <0|1> (default: 1)~n"),
	io:format("  hw_aes <0|1> (default: 1)~n"),
	erlang:halt().

run_benchmark(RandomXState) ->
	run_benchmark(RandomXState, ar_mine_randomx:jit(),
		ar_mine_randomx:large_pages(), ar_mine_randomx:hardware_aes()).

run_benchmark(RandomXState, JIT, LargePages, HardwareAES) ->
	NonceLimiterOutput = crypto:strong_rand_bytes(32),
	Seed = crypto:strong_rand_bytes(32),
	MiningAddr = crypto:strong_rand_bytes(32),
	Iterations = 1000,
	{H0Time, _} = timer:tc(fun() ->
		lists:foreach(
			fun(_) ->
				PartitionNumber = rand:uniform(1000),
				Data = << NonceLimiterOutput:32/binary,
					PartitionNumber:256, Seed:32/binary, MiningAddr/binary >>,
				ar_mine_randomx:hash(RandomXState, Data, JIT, LargePages, HardwareAES)
			end,
			lists:seq(1, Iterations))
		end),
	H0Microseconds = H0Time / Iterations,

	H0 = crypto:strong_rand_bytes(32),
	Chunk = crypto:strong_rand_bytes(?DATA_CHUNK_SIZE),
	{H1Time, _} = timer:tc(fun() ->
		lists:foreach(
			fun(_) ->
				Nonce = rand:uniform(1000),
				Preimage = crypto:hash(sha256, << H0:32/binary, Nonce:64, Chunk/binary >>),
				crypto:hash(sha256, << H0:32/binary, Preimage/binary >>)
			end,
			lists:seq(1, Iterations))
		end),
	H1Microseconds = H1Time / Iterations,

	{H0Microseconds, H1Microseconds}.
]]>
</file>

<file path="/Users/peng.li/workspace/arweave/apps/arweave/src/ar_bench_packing.erl">
<![CDATA[
-module(ar_bench_packing).

-export([run_benchmark_from_cli/1]).

-include_lib("arweave/include/ar.hrl").
-include_lib("arweave/include/ar_consensus.hrl").
-include_lib("kernel/include/file.hrl").

-record(test_config, {
	test,
	num_workers,
	total_megabytes,
	jit,
	large_pages,
	hardware_aes,
	packing_difficulty,
	rounds,
	root,
	src_address,
	dst_address,
	randomx_state,
	input_file,
	output_file
}).

-define(VALID_TESTS, #{
	pack_legacy => {false, fun pack_legacy_chunks/4},
	pack_composite => {false, fun pack_composite_chunks/4},
	erl_repack_legacy => {true, fun erl_repack_legacy_chunks/4},
	nif_repack_legacy => {true, fun nif_repack_legacy_chunks/4},
	nif_repack_composite => {true, fun nif_repack_composite_chunks/4}
}).

run_benchmark_from_cli(Args) ->
	Test = list_to_atom(get_flag_value(Args, "test", "pack_legacy")),
	JIT = list_to_integer(get_flag_value(Args, "jit", "1")),
	LargePages = list_to_integer(get_flag_value(Args, "large_pages", "1")),
	HardwareAES = list_to_integer(get_flag_value(Args, "hw_aes", "1")),
	PackingDifficulty = list_to_integer(get_flag_value(Args, "pdiff", "1")),
	Rounds = list_to_integer(get_flag_value(Args, "rounds",
		integer_to_list(?COMPOSITE_PACKING_ROUND_COUNT))),
	run_benchmark(Test, JIT, LargePages, HardwareAES, PackingDifficulty, Rounds).

get_flag_value([], _, DefaultValue) ->
	DefaultValue;
get_flag_value([Flag | [Value | _Tail]], TargetFlag, _DefaultValue) when Flag == TargetFlag ->
	Value;
get_flag_value([_ | Tail], TargetFlag, DefaultValue) ->
	get_flag_value(Tail, TargetFlag, DefaultValue).

show_help() ->
	io:format("~nUsage: benchmark-packing [options]~n"),
	io:format("Options:~n"),
	io:format("  test <test> (default: pack_legacy)~n"),
	io:format("  mb <megabytes> (default: 16)~n"),
	io:format("  jit <0|1> (default: 1)~n"),
	io:format("  large_pages <0|1> (default: 1)~n"),
	io:format("  hw_aes <0|1> (default: 1)~n"),
	io:format("  pdiff <number> (default: 1)~n"),
	io:format("  rounds <number> (default: 10)~n"),
	lists:foreach(fun(Test) -> io:format("  ~p~n", [Test]) end, maps:keys(?VALID_TESTS)),
	erlang:halt().

run_benchmark(Test, JIT, LargePages, HardwareAES, PackingDifficulty, Rounds) ->
	timer:sleep(3000),
	ets:new(offsets, [set, named_table, public]),
	EncodedRoot = <<"OIgTTxuEPklMR47Ho8VWnNr1Uh6TNjzxwIs38yuqBK0">>,
	Root = ar_util:decode(EncodedRoot),
	EncodedSrcAddress = <<"mvK6e65dcD6XNYDHUVxMa7-d6wVP535Ummtvb8OCUtQ">>,
	SrcAddress = ar_util:decode(EncodedSrcAddress),
	EncodedDstAddress = <<"ymvkTAt6DVo0LaV3SH4TPLvzCmn5TIqvCcv1pHWt2Zs">>,
	DstAddress = ar_util:decode(EncodedDstAddress),

	NumWorkers = erlang:system_info(dirty_cpu_schedulers_online),

	TotalMegaBytes = (1024 div NumWorkers) * NumWorkers,

	Config = #test_config{
		test = Test,
		num_workers = NumWorkers,
		total_megabytes = TotalMegaBytes,
		jit = JIT,
		large_pages = LargePages,
		hardware_aes = HardwareAES,
		packing_difficulty = PackingDifficulty,
		rounds = Rounds,
		root = Root,
		src_address = SrcAddress,
		dst_address = DstAddress
	},

	io:format("~nBenchmark settings:~n"),
	io:format("~12s: ~p~n", ["Test", Test]),
	io:format("~12s: ~p~n", ["Data (MB)", TotalMegaBytes]),
	io:format("~12s: ~p~n", ["Cores Used", NumWorkers]),
	io:format("~12s: ~p~n", ["JIT", JIT]),
	io:format("~12s: ~p~n", ["Large Pages", LargePages]),
	io:format("~12s: ~p~n", ["HW AES", HardwareAES]),
	io:format("~nBenchmark settings (composite only):~n"),
	io:format("~12s: ~p~n", ["pdiff", PackingDifficulty]),
	io:format("~12s: ~p~n", ["rounds", Rounds]),
	io:format("~n"),

	generate_input(Config),

	case lists:member(Test, maps:keys(?VALID_TESTS)) of
		true ->
			run_benchmark(Config);
		false ->
			show_help()
	end,

	Init = ar_bench_timer:get_total({init}) / 1000000,
	Total = ar_bench_timer:get_total({wall}) / 1000000,

	File = open_file("benchmark.results.csv", [append]),

	%% Write the CSV string to the file
	Output = io_lib:format("~p, ~p, ~p, ~p, ~p, ~p, ~p, ~p, ~p, ~p~n", [
		erlang:system_time() div 1000000000,
		Test, TotalMegaBytes, JIT, LargePages, HardwareAES,
		PackingDifficulty, Rounds,
		Init, Total]),
	
	file:write(File, Output),
	file:close(File),

	Label = "Chunks Processed",

	Chunks = (TotalMegaBytes * ?MiB) div ?DATA_CHUNK_SIZE,
	TimePerChunk = (Total / Chunks) * 1000,
	TimePerChunkPerCore = TimePerChunk * NumWorkers,
	ChunksPerSecond = Chunks / Total,
	ChunksPerSecondPerCore = ChunksPerSecond / NumWorkers,

	io:format("~nBenchmark results:~n"),
	io:format("~28s: ~p~n", [Label, Chunks]),
	io:format("~28s: ~.2f~n", ["Total Time (s)", Total]),
	io:format("~28s: ~.2f~n", ["Init Time (s)", Init]),
	io:format("~28s: ~.2f~n", ["Time Per Chunk (ms)", TimePerChunk]),
	io:format("~28s: ~.2f~n", ["Time Per Chunk Per Core (ms)", TimePerChunkPerCore]),
	io:format("~28s: ~p~n", ["Chunks Per Second", floor(ChunksPerSecond)]),
	io:format("~28s: ~p~n", ["Chunks Per Second Per Core", floor(ChunksPerSecondPerCore)]).

%% --------------------------------------------------------------------------------------------
%% Write Input files
%% --------------------------------------------------------------------------------------------
is_repack_test(Test) ->
	{IsRepackTest, _} = maps:get(Test, ?VALID_TESTS),
	IsRepackTest.

output_filename(Config) ->
	#test_config{
		test = Test,
		total_megabytes = TotalMegaBytes,
		jit = JIT,
		large_pages = LargePages,
		hardware_aes = HardwareAES
	} = Config,
	Permutation = {TotalMegaBytes, JIT, LargePages, HardwareAES},
	%% convert the Permutation tuple to a list of strings so that we can join them with a dot
	StringList = lists:map(fun(E) -> integer_to_list(E) end, tuple_to_list(Permutation)),
	io_lib:format("benchmark.output.~s.~p", [string:join(StringList, "."), Test]).
unpacked_filename(TotalMegaBytes) ->
	io_lib:format("benchmark.input.~p.unpacked", [TotalMegaBytes]).
packed_filename(TotalMegaBytes) ->
	io_lib:format("benchmark.input.~p.packed", [TotalMegaBytes]).

generate_input(Config) ->
	#test_config{ total_megabytes = TotalMegaBytes } = Config,
	TotalBytes = TotalMegaBytes * ?MiB,

	UnpackedFilename = unpacked_filename(TotalMegaBytes),
	case file:read_file_info(UnpackedFilename) of
		{ok, FileInfo1} ->
			if
				FileInfo1#file_info.size == TotalBytes ->
					ok;
				true ->
					file:delete(UnpackedFilename),
					write_random_data(UnpackedFilename, TotalBytes)
			end;
		{error, _} ->
			write_random_data(UnpackedFilename, TotalBytes)
	end,

	PackedFilename = packed_filename(TotalMegaBytes),
	case file:read_file_info(PackedFilename) of
		{ok, FileInfo2} ->
			%% If the file already exists and is the correct size, we don't need to do anything
			if
				FileInfo2#file_info.size == TotalBytes ->
					ok;
				true ->
					file:delete(PackedFilename),
					write_packed_data(Config, UnpackedFilename, PackedFilename)
			end;
		{error, _} ->
			write_packed_data(Config, UnpackedFilename, PackedFilename)
	end.

write_random_data(UnpackedFilename, TotalBytes) ->
	io:format("Generating input file: ~s~n", [UnpackedFilename]),
	File = open_file(UnpackedFilename, [write, binary, raw]),
	write_chunks(File, TotalBytes),
	file:close(File).
write_chunks(File, TotalBytes) ->
	ChunkSize = 1024*1024, % 1MB
	RemainingBytes = TotalBytes,
	write_chunks_loop(File, RemainingBytes, ChunkSize).
write_chunks_loop(_File, 0, _) ->
	ok;
write_chunks_loop(File, RemainingBytes, ChunkSize) ->
	BytesToWrite = min(RemainingBytes, ChunkSize),
	Data = crypto:strong_rand_bytes(BytesToWrite),
	file:write(File, Data),
	write_chunks_loop(File, RemainingBytes - BytesToWrite, ChunkSize).

write_packed_data(Config, UnpackedFilename, PackedFilename) ->
	io:format("Generating input file: ~s~n", [PackedFilename]),
	{ok, RandomXState} = {ok, RandomXState} = init_randomx_state(Config),

	UnpackedFileHandle = open_file(UnpackedFilename, [read, binary]),
	PackedFileHandle = open_file(PackedFilename, [write, binary]),

	test(Config#test_config{
		test = pack_legacy,
		randomx_state = RandomXState,
		input_file = UnpackedFileHandle,
		output_file = PackedFileHandle
	}),
	
	file:close(PackedFileHandle),
	file:close(UnpackedFileHandle).

%% --------------------------------------------------------------------------------------------
%% Test Runners
%% --------------------------------------------------------------------------------------------

run_benchmark(Config) ->
	#test_config{
		test = Test,
		total_megabytes = TotalMegaBytes
	} = Config,
	
	Config2 = case is_repack_test(Test) of
		true ->
			Config#test_config{
				input_file = open_file(packed_filename(TotalMegaBytes), [read, binary]),
				output_file = open_file(output_filename(Config), [write, binary])
			};
		false ->
			Config#test_config{
				input_file = open_file(unpacked_filename(TotalMegaBytes), [read, binary]),
				output_file = open_file(output_filename(Config), [write, binary])
			}
	end,

	{ok, RandomXState} = init_randomx_state(Config),

	run_test(Config2#test_config{randomx_state = RandomXState}).

init_randomx_state(Config) ->
	#test_config{
		test = Test,
		num_workers = NumWorkers,
		jit = JIT,
		large_pages = LargePages
	} = Config,
	case lists:member(Test, [pack_composite, nif_repack_composite]) of
		true ->
			ar_bench_timer:record({init},
				fun ar_rx4096_nif:rx4096_init_nif/5,
					[?RANDOMX_PACKING_KEY, ?RANDOMX_HASHING_MODE_FAST, 
						JIT, LargePages, NumWorkers]);
		false ->
			ar_bench_timer:record({init},
				fun ar_rx512_nif:rx512_init_nif/5,
					[?RANDOMX_PACKING_KEY, ?RANDOMX_HASHING_MODE_FAST,
						JIT, LargePages, NumWorkers])
	end.

run_test(Config) ->
	#test_config{
		input_file = InputFileHandle,
		output_file = OutputFileHandle
	} = Config,

	io:format("packing..."),
	ar_bench_timer:record({wall}, fun test/1, [Config]),

	file:close(InputFileHandle),
	file:close(OutputFileHandle).

%% For now this just encrypts each chunk without adding the offset hash
test(Config) ->
	#test_config{
		test = Test,
		total_megabytes = TotalMegaBytes,
		num_workers = NumWorkers
	} = Config,
	TotalBytes = TotalMegaBytes * ?MiB,
	%% Spin up NumWorkers threads each responsible for a fraction of the file
	WorkerSize = TotalBytes div NumWorkers,
	{_, WorkerFun} = maps:get(Test, ?VALID_TESTS),
	Workers = [spawn_monitor(
		fun() -> worker(
			N,
			Config, 
			WorkerFun,
			WorkerSize * (N - 1),
			WorkerSize
		) end) || N <- lists:seq(1, NumWorkers)],
	%% Wait for all workers to finish
	[
		receive
			{'DOWN', Ref, process, Pid, _Result} -> erlang:demonitor(Ref), ok
		after 
			60000 -> timeout
		end || {Pid, Ref} <- Workers
	],
	io:format("~n").

worker(WorkerID, Config, WorkerFun, Offset, Size) ->
	ar_bench_timer:record({total, WorkerID}, WorkerFun, [
			WorkerID,
			Config,
			Offset,
			Size
		]),
	exit(normal).

%% --------------------------------------------------------------------------------------------
%% Baseline Packing Test
%% --------------------------------------------------------------------------------------------
pack_legacy_chunks(_WorkerID, _Config, _Offset, Size) when Size =< 0 ->
	ok;
pack_legacy_chunks(WorkerID, Config, Offset, Size) ->
	#test_config{
		randomx_state = RandomXState,
		jit = JIT,
		large_pages = LargePages,
		hardware_aes = HardwareAES,
		input_file = UnpackedFileHandle,
		output_file = PackedFileHandle,
		root = Root,
		dst_address = DstAddress
	} = Config,
	ChunkSize = min(Size, ?DATA_CHUNK_SIZE),
	{spora_2_6, Key} = ar_packing_server:chunk_key({spora_2_6, DstAddress}, Offset, Root),
	ReadResult = file:pread(UnpackedFileHandle, Offset, ChunkSize),
	RemainingSize = case ReadResult of
		{ok, UnpackedChunk} ->
			{ok, PackedChunk} = ar_rx512_nif:rx512_encrypt_chunk_nif(
				RandomXState, Key, UnpackedChunk, ?RANDOMX_PACKING_ROUNDS_2_6,
				JIT, LargePages, HardwareAES),
			file:pwrite(PackedFileHandle, Offset, PackedChunk),
			(Size - ChunkSize);
		eof ->
			0;
		{error, Reason} ->
			io:format("Error reading file: ~p~n", [Reason]),
			0
	end,
	pack_legacy_chunks(WorkerID, Config, Offset+ChunkSize, RemainingSize).

%% --------------------------------------------------------------------------------------------
%% Baseline Packing 2.8 Test
%% --------------------------------------------------------------------------------------------
% TODO diff other than 1

pack_composite_chunks(_WorkerID, _Config, _Offset, Size) when Size =< 0 ->
	ok;
pack_composite_chunks(WorkerID, Config, Offset, Size) ->
	#test_config{
		randomx_state = RandomXState,
		jit = JIT,
		large_pages = LargePages,
		hardware_aes = HardwareAES,
		input_file = UnpackedFileHandle,
		output_file = PackedFileHandle,
		root = Root,
		dst_address = DstAddress,
		packing_difficulty = PackingDifficulty,
		rounds = Rounds
	} = Config,
	ChunkSize = min(Size, ?DATA_CHUNK_SIZE),
	{composite, Key} = ar_packing_server:chunk_key({composite, DstAddress, PackingDifficulty}, Offset, Root),
	ReadResult = file:pread(UnpackedFileHandle, Offset, ChunkSize),
	RemainingSize = case ReadResult of
		{ok, UnpackedChunk} ->
			{ok, PackedChunk} = ar_rx4096_nif:rx4096_encrypt_composite_chunk_nif(
				RandomXState, Key, UnpackedChunk,
				JIT, LargePages, HardwareAES,
				Rounds, PackingDifficulty, ?COMPOSITE_PACKING_SUB_CHUNK_COUNT),
			file:pwrite(PackedFileHandle, Offset, PackedChunk),
			(Size - ChunkSize);
		eof ->
			0;
		{error, Reason} ->
			io:format("Error reading file: ~p~n", [Reason]),
			0
	end,
	pack_composite_chunks(WorkerID, Config, Offset+ChunkSize, RemainingSize).

%% --------------------------------------------------------------------------------------------
%% Baseline Repacking Test
%% --------------------------------------------------------------------------------------------
erl_repack_legacy_chunks(_WorkerID, _Config, _Offset, Size) when Size =< 0 ->
	ok;
erl_repack_legacy_chunks(WorkerID, Config, Offset, Size) ->
	#test_config{
		randomx_state = RandomXState,
		jit = JIT,
		large_pages = LargePages,
		hardware_aes = HardwareAES,
		input_file = PackedFileHandle,
		output_file = RepackedFileHandle,
		root = Root,
		src_address = SrcAddress,
		dst_address = DstAddress
	} = Config,
	ChunkSize = min(Size, ?DATA_CHUNK_SIZE),
	{spora_2_6, UnpackKey} = ar_packing_server:chunk_key({spora_2_6, SrcAddress}, Offset, Root),
	{spora_2_6, PackKey} = ar_packing_server:chunk_key({spora_2_6, DstAddress}, Offset, Root),
	ReadResult = file:pread(PackedFileHandle, Offset, ChunkSize),
	RemainingSize = case ReadResult of
		{ok, PackedChunk} ->
			{ok, UnpackedChunk} = ar_rx512_nif:rx512_decrypt_chunk_nif(
				RandomXState, UnpackKey, PackedChunk, ChunkSize, ?RANDOMX_PACKING_ROUNDS_2_6,
				JIT, LargePages, HardwareAES),
			{ok, RepackedChunk} =ar_rx512_nif:rx512_encrypt_chunk_nif(
				RandomXState, PackKey, UnpackedChunk, ?RANDOMX_PACKING_ROUNDS_2_6,
				JIT, LargePages, HardwareAES),	
			file:pwrite(RepackedFileHandle, Offset, RepackedChunk),
			(Size - ChunkSize);
		eof ->
			0;
		{error, Reason} ->
			io:format("Error reading file: ~p~n", [Reason]),
			0
	end,
	erl_repack_legacy_chunks(WorkerID, Config, Offset+ChunkSize, RemainingSize).

%% --------------------------------------------------------------------------------------------
%% NIF Repacking Test
%% --------------------------------------------------------------------------------------------
nif_repack_legacy_chunks(_WorkerID, _Config, _Offset, Size) when Size =< 0 ->
	ok;
nif_repack_legacy_chunks(WorkerID, Config, Offset, Size) ->
	#test_config{
		randomx_state = RandomXState,
		jit = JIT,
		large_pages = LargePages,
		hardware_aes = HardwareAES,
		input_file = PackedFileHandle,
		output_file = RepackedFileHandle,
		root = Root,
		src_address = SrcAddress,
		dst_address = DstAddress
	} = Config,
	ChunkSize = min(Size, ?DATA_CHUNK_SIZE),
	{spora_2_6, UnpackKey} = ar_packing_server:chunk_key({spora_2_6, SrcAddress}, Offset, Root),
	{spora_2_6, PackKey} = ar_packing_server:chunk_key({spora_2_6, DstAddress}, Offset, Root),
	ReadResult = file:pread(PackedFileHandle, Offset, ChunkSize),
	RemainingSize = case ReadResult of
		{ok, PackedChunk} ->
			{ok, RepackedChunk, _} = ar_rx512_nif:rx512_reencrypt_chunk_nif(
				RandomXState, UnpackKey, PackKey, PackedChunk, ChunkSize,
				?RANDOMX_PACKING_ROUNDS_2_6, ?RANDOMX_PACKING_ROUNDS_2_6,
				JIT, LargePages, HardwareAES),
			file:pwrite(RepackedFileHandle, Offset, RepackedChunk),
			(Size - ChunkSize);
		eof ->
			0;
		{error, Reason} ->
			io:format("Error reading file: ~p~n", [Reason]),
			0
	end,
	nif_repack_legacy_chunks(WorkerID, Config, Offset+ChunkSize, RemainingSize).

nif_repack_composite_chunks(_WorkerID, _Config, _Offset, Size) when Size =< 0 ->
	ok;
nif_repack_composite_chunks(WorkerID, Config, Offset, Size) ->
	#test_config{
		randomx_state = RandomXState,
		jit = JIT,
		large_pages = LargePages,
		hardware_aes = HardwareAES,
		input_file = PackedFileHandle,
		output_file = RepackedFileHandle,
		root = Root,
		src_address = SrcAddress,
		dst_address = DstAddress,
		packing_difficulty = PackingDifficulty,
		rounds = Rounds
	} = Config,
	ChunkSize = min(Size, ?DATA_CHUNK_SIZE),
	{composite, UnpackKey} = ar_packing_server:chunk_key({composite, SrcAddress, PackingDifficulty}, Offset, Root),
	{composite, PackKey} = ar_packing_server:chunk_key({composite, DstAddress, PackingDifficulty}, Offset, Root),
	ReadResult = file:pread(PackedFileHandle, Offset, ChunkSize),
	RemainingSize = case ReadResult of
		{ok, PackedChunk} ->
			{ok, RepackedChunk, _} = ar_rx4096_nif:rx4096_reencrypt_composite_chunk_nif(
				RandomXState, UnpackKey, PackKey, PackedChunk, JIT, LargePages, HardwareAES,
				Rounds, Rounds, PackingDifficulty, PackingDifficulty,
				?COMPOSITE_PACKING_SUB_CHUNK_COUNT, ?COMPOSITE_PACKING_SUB_CHUNK_COUNT),
			
			file:pwrite(RepackedFileHandle, Offset, RepackedChunk),
			(Size - ChunkSize);
		eof ->
			0;
		{error, Reason} ->
			io:format("Error reading file: ~p~n", [Reason]),
			0
	end,
	nif_repack_composite_chunks(WorkerID, Config, Offset+ChunkSize, RemainingSize).

%% --------------------------------------------------------------------------------------------
%% Helpers
%% --------------------------------------------------------------------------------------------
open_file(Filename, Options) ->
	case file:open(Filename, Options) of
		{ok, File} ->
			File;
		{error, Reason} ->
			io:format("Error opening ~s: ~p~n", [Filename, Reason]),
			show_help()
	end.

]]>
</file>

<file path="/Users/peng.li/workspace/arweave/apps/arweave/src/ar_bench_timer.erl">
<![CDATA[
-module(ar_bench_timer).

-export([initialize/0, reset/0, record/3, start/1, stop/1, get_timing_data/0, print_timing_data/0, get_total/1, get_max/1, get_min/1, get_avg/1]).

-include_lib("arweave/include/ar.hrl").
-include_lib("arweave/include/ar_vdf.hrl").
-include_lib("arweave/include/ar_consensus.hrl").

record(Key, Fun, Args) ->
    {Time, Result} = timer:tc(Fun, Args),
    update_total(Key, Time),
    Result.

start(Key) ->
	StartTime = erlang:timestamp(),
	ets:insert(start_time, {real_key(Key), StartTime}).

stop(Key) ->
	case ets:lookup(start_time, real_key(Key)) of
		[{_, StartTime}] ->
			EndTime = erlang:timestamp(),
			ElapsedTime = timer:now_diff(EndTime, StartTime),
			% io:format("Elapsed ~p: ~p -> ~p = ~p~n", [Key, StartTime, EndTime, ElapsedTime]),
			update_total(Key, ElapsedTime),
			ElapsedTime;
		[] ->
			% Key not found, throw an error
			{error, {not_started, Key}}
	end.

update_total(Key, ElapsedTime) ->
	ets:update_counter(total_time, real_key(Key), {2, ElapsedTime}, {real_key(Key), 0}).

get_total([]) ->
	0;
get_total(Times) when is_list(Times) ->
	lists:sum(Times);
get_total(Key) ->
    get_total(get_times(Key)).

get_max([]) ->
	0;
get_max(Times) when is_list(Times) ->
    lists:max(Times);
get_max(Key) ->
	get_max(get_times(Key)).

get_min([]) ->
	0;
get_min(Times) when is_list(Times) ->
    lists:min(Times);
get_min(Key) ->
    get_min(get_times(Key)).

get_avg([]) ->
	0;
get_avg(Times) when is_list(Times) ->
	TotalTime = lists:sum(Times),
    case length(Times) of
        0 -> 0;
        N -> TotalTime / N
    end;
get_avg(Key) ->
	get_avg(get_times(Key)).
	
get_times(Key) ->
	[Match || [Match] <- ets:match(total_time, {{Key, '_'}, '$1'})].
get_timing_keys() ->
    Keys = [Key || {{Key, _PID}, _Value} <- get_timing_data()],
    UniqueKeys = sets:to_list(sets:from_list(Keys)),
	UniqueKeys.
get_timing_data() ->
    ets:tab2list(total_time).
print_timing_data() ->
	lists:foreach(fun(Key) ->
			Seconds = get_total(Key) / 1000000,
			?LOG_ERROR("~p: ~p", [Key, Seconds])
		end, get_timing_keys()).

reset() ->
	ets:delete_all_objects(total_time),
	ets:delete_all_objects(start_time).

initialize() ->
    ets:new(total_time, [set, named_table, public]),
	ets:new(start_time, [set, named_table, public]).

real_key(Key) ->
	{Key, self()}.



]]>
</file>

<file path="/Users/peng.li/workspace/arweave/apps/arweave/src/ar_bench_vdf.erl">
<![CDATA[
-module(ar_bench_vdf).

-export([run_benchmark/0]).

-include_lib("arweave/include/ar_vdf.hrl").

run_benchmark() ->
	Input = crypto:strong_rand_bytes(32),
	{Time, _} = timer:tc(fun() -> ar_vdf:compute2(1, Input, ?VDF_DIFFICULTY) end),
	io:format("~n~nVDF step computed in ~.2f seconds.~n~n", [Time / 1000000]),
	case Time > 1150000 of
		true ->
			io:format("WARNING: your VDF computation speed is low - consider fetching "
					"VDF outputs from an external source (see vdf_server_trusted_peer "
					"and vdf_client_peer command line parameters).~n~n");
		false ->
			ok
	end,
	Time.
]]>
</file>

<file path="/Users/peng.li/workspace/arweave/apps/arweave/src/ar_blacklist_middleware.erl">
<![CDATA[
-module(ar_blacklist_middleware).

-behaviour(cowboy_middleware).

-export([start/0, execute/2, reset/0, reset_rate_limit/3,
		ban_peer/2, is_peer_banned/1, cleanup_ban/1, decrement_ip_addr/2]).

-include_lib("arweave/include/ar.hrl").
-include_lib("arweave/include/ar_config.hrl").
-include_lib("arweave/include/ar_blacklist_middleware.hrl").
-include_lib("eunit/include/eunit.hrl").

execute(Req, Env) ->
	IPAddr = requesting_ip_addr(Req),
	{ok, Config} = application:get_env(arweave, config),
	case lists:member(blacklist, Config#config.disable) of
		true ->
			{ok, Req, Env};
		_ ->
			LocalIPs = [peer_to_ip_addr(Peer) || Peer <- Config#config.local_peers],
			case lists:member(IPAddr, LocalIPs) of
				true ->
					{ok, Req, Env};
				false ->
					case increment_ip_addr(IPAddr, Req) of
						block -> {stop, blacklisted(Req)};
						pass -> {ok, Req, Env}
					end
			end
	end.

start() ->
	?LOG_INFO([{event, ar_blacklist_middleware_start}]),
	{ok, _} =
		timer:apply_after(
			?BAN_CLEANUP_INTERVAL,
			?MODULE,
			cleanup_ban,
			[ets:whereis(?MODULE)]
		),
	ok.

%% Ban a peer completely for TTLSeconds seoncds. Since we cannot trust the port,
%% we ban the whole IP address.
ban_peer(Peer, TTLSeconds) ->
	?LOG_DEBUG([{event, ban_peer}, {peer, ar_util:format_peer(Peer)}, {seconds, TTLSeconds}]),
	Key = {ban, peer_to_ip_addr(Peer)},
	Expires = os:system_time(seconds) + TTLSeconds,
	ets:insert(?MODULE, {Key, Expires}).

is_peer_banned(Peer) ->
	Key = {ban, peer_to_ip_addr(Peer)},
	case ets:lookup(?MODULE, Key) of
		[] -> not_banned;
		[_] -> banned
	end.

cleanup_ban(TableID) ->
	case ets:whereis(?MODULE) of
		TableID ->
			Now = os:system_time(seconds),
			Folder = fun
				({{ban, _} = Key, Expires}, Acc) when Expires < Now ->
					[Key | Acc];
				(_, Acc) ->
					Acc
			end,
			RemoveKeys = ets:foldl(Folder, [], ?MODULE),
			Delete = fun(Key) -> ets:delete(?MODULE, Key) end,
			lists:foreach(Delete, RemoveKeys),
			{ok, _} =
				timer:apply_after(
					?BAN_CLEANUP_INTERVAL,
					?MODULE,
					cleanup_ban,
					[TableID]
				);
		_ ->
			table_owner_died
	end.

%private functions
blacklisted(Req) ->
	cowboy_req:reply(
		429,
		#{<<"connection">> => <<"close">>},
		<<"Too Many Requests">>,
		Req
	).

reset() ->
	true = ets:delete_all_objects(?MODULE),
	ok.

reset_rate_limit(TableID, IPAddr, Path) ->
	case ets:whereis(?MODULE) of
		TableID ->
			ets:delete(?MODULE, {rate_limit, IPAddr, Path});
		_ ->
			table_owner_died
	end.

increment_ip_addr(IPAddr, Req) ->
	case ets:whereis(?MODULE) of 
		undefined -> pass;
		_ -> update_ip_addr(IPAddr, Req, 1)
	end.

decrement_ip_addr(IPAddr, Req) ->
	case ets:whereis(?MODULE) of 
		undefined -> pass;
		_ -> update_ip_addr(IPAddr, Req, -1)
	end.

update_ip_addr(IPAddr, Req, Delta) ->
	{PathKey, Limit}  = get_key_limit(IPAddr, Req),
	%% Divide by 2 as the throttle period is 30 seconds.
	RequestLimit = Limit div 2,
	Key = {rate_limit, IPAddr, PathKey},
	case ets:update_counter(?MODULE, Key, {2, Delta}, {Key, 0}) of
		1 ->
			timer:apply_after(
				?THROTTLE_PERIOD,
				?MODULE,
				reset_rate_limit,
				[ets:whereis(?MODULE), IPAddr, PathKey]
			),
			pass;
		Count when Count =< RequestLimit ->
			pass;
		_ ->
			block
	end.

requesting_ip_addr(Req) ->
	{IPAddr, _} = cowboy_req:peer(Req),
	IPAddr.

peer_to_ip_addr({A, B, C, D, _}) -> {A, B, C, D}.

get_key_limit(IPAddr, Req) ->
	Path = ar_http_iface_server:split_path(cowboy_req:path(Req)),
	{ok, Config} = application:get_env(arweave, config),
	Map = maps:get(IPAddr, Config#config.requests_per_minute_limit_by_ip, #{}),
	?RPM_BY_PATH(Path, Map)().

]]>
</file>

<file path="/Users/peng.li/workspace/arweave/apps/arweave/src/ar_block_cache.erl">
<![CDATA[
%%% @doc The module maintains a DAG of blocks that have passed the PoW validation, in ETS.
%%% NOTE It is not safe to call functions which modify the state from different processes.
-module(ar_block_cache).

-export([new/2, initialize_from_list/2, add/2, mark_nonce_limiter_validated/2,
		add_validated/2,
		mark_tip/2, get/2, get_earliest_not_validated_from_longest_chain/1,
		get_longest_chain_cache/1,
		get_block_and_status/2, remove/2, get_checkpoint_block/1, prune/2,
		get_by_solution_hash/5, is_known_solution_hash/2,
		get_siblings/2, get_fork_blocks/2, update_timestamp/3]).

-include_lib("arweave/include/ar.hrl").
-include_lib("eunit/include/eunit.hrl").

%% The expiration time in seconds for every "alternative" block (a block with non-unique
%% solution).
-define(ALTERNATIVE_BLOCK_EXPIRATION_TIME_SECONDS, 5).

%% @doc Block validation status
%% on_chain: block is validated and belongs to the tip fork
%% validated: block is validated but does not belong to the tip fork
%% not_validated: block is not validated yet
%% none: null status

%% @doc ETS table: block_cache
%% {block, BlockHash} => {#block{}, block_status(), Timestamp, set(Children)}
%%   - Children is a set of all blocks that have this block as their previous block. Children is
%%     used to track branches in the chain that fork off this block (i.e. they are DAG children)
%% max_cdiff => {CDiff, BlockHash}
%%   - maximum cumulative difficulty encountered and its BlockHash. This is used to determine
%%     whether we need to switch from the current tip to a fork tip.
%% {solution, SolutionHash} => set(BlockHash)
%%   - all blocks with the same solution hash
%% longest_chain => [{BlockHash, [TXIDs]}]
%%  - the top ?STORE_BLOCKS_BEHIND_CURRENT blocks of the longest chain
%% tip -> BlockHash
%%   - curent block chain tip
%% links -> gb_set({Height, BlockHash})
%%   - all blocks in the cache sorted by height. This is used when pruning the cache and
%%     discarding all blocks below a certain height (and all off-chain children of those blocks
%%     regardless of their height)

%%%===================================================================
%%% Public API.
%%%===================================================================

%% @doc Create a cache, initialize it with the given block. The block is marked as on-chain
%% and as a tip block.
new(Tab, B) ->
	#block{ indep_hash = H, hash = SolutionH, cumulative_diff = CDiff, height = Height } = B,
	ets:delete_all_objects(Tab),
	ar_ignore_registry:add(H),
	insert(Tab, [
		{max_cdiff, {CDiff, H}},
		{links, gb_sets:from_list([{Height, H}])},
		{{solution, SolutionH}, sets:from_list([H])},
		{tip, H},
		{{block, H}, {B, on_chain, erlang:timestamp(), sets:new()}}
	]).

%% @doc Initialize a cache from the given list of validated blocks. Mark the latest
%% block as the tip block. The given blocks must be sorted from newest to oldest.
initialize_from_list(Tab, [B]) ->
	new(Tab, B);
initialize_from_list(Tab, [#block{ indep_hash = H } = B | Blocks]) ->
	initialize_from_list(Tab, Blocks),
	add_validated(Tab, B),
	mark_tip(Tab, H).

%% @doc Add a block to the cache. The block is marked as not validated yet.
%% If the block is already present in the cache and has not been yet validated, it is
%% overwritten. If the block is validated, we do nothing and issue a warning.
add(Tab,
		#block{
			indep_hash = H,
			hash = SolutionH,
			previous_block = PrevH,
			height = Height
		} = B) ->
	case ets:lookup(Tab, {block, H}) of
		[] ->
			ar_ignore_registry:add(H),
			RemainingHs = remove_expired_alternative_blocks(Tab, SolutionH),
			SolutionSet = sets:from_list([H | RemainingHs]),
			[{_, Set}] = ets:lookup(Tab, links),
			[{_, {PrevB, PrevStatus, PrevTimestamp, Children}}] = ets:lookup(Tab, {block, PrevH}),
			Set2 = gb_sets:insert({Height, H}, Set),
			Status = {not_validated, awaiting_nonce_limiter_validation},
			%% If CDiff > MaxCDiff it means this block belongs to the heaviest fork we're aware
			%% of. If our current tip is not on this fork, ar_node_worker may switch to this fork.
			insert(Tab, [
				{max_cdiff, maybe_increase_max_cdiff(Tab, B, Status)},
				{links, Set2},
				{{solution, SolutionH}, SolutionSet},
				{{block, H}, {B, Status, erlang:timestamp(), sets:new()}},
				{{block, PrevH},
						{PrevB, PrevStatus, PrevTimestamp, sets:add_element(H, Children)}}
			]);
		[{_, {_B, {not_validated, _} = CurrentStatus, CurrentTimestamp, Children}}] ->
			insert(Tab, {{block, H}, {B, CurrentStatus, CurrentTimestamp, Children}});
		_ ->
			?LOG_WARNING([{event, attempt_to_update_already_validated_cached_block},
					{h, ar_util:encode(H)}, {height, Height},
					{previous_block, ar_util:encode(PrevH)}]),
			ok
	end.

%% @doc Check all blocks that share the same solution and remove those that expired.
remove_expired_alternative_blocks(Tab, SolutionH) ->
	SolutionSet =
		case ets:lookup(Tab, {solution, SolutionH}) of
			[] ->
				sets:new();
			[{_, SolutionSet2}] ->
				SolutionSet2
		end,
	remove_expired_alternative_blocks2(Tab, sets:to_list(SolutionSet)).

remove_expired_alternative_blocks2(_Tab, []) ->
	[];
remove_expired_alternative_blocks2(Tab, [H | Hs]) ->
	[{_, {_B, Status, Timestamp, Children}}] = ets:lookup(Tab, {block, H}),
	case Status of
		on_chain ->
			[H | remove_expired_alternative_blocks2(Tab, Hs)];
		_ ->
			LifetimeSeconds = get_alternative_block_lifetime(Tab, Children),
			{MegaSecs, Secs, MicroSecs} = Timestamp,
			ExpirationTimestamp = {MegaSecs, Secs + LifetimeSeconds, MicroSecs},
			case timer:now_diff(erlang:timestamp(), ExpirationTimestamp) >= 0 of
				true ->
					remove(Tab, H),
					remove_expired_alternative_blocks2(Tab, Hs);
				false ->
					[H | remove_expired_alternative_blocks2(Tab, Hs)]
			end
	end.

get_alternative_block_lifetime(Tab, Children) ->
	ForkLen = get_fork_length(Tab, sets:to_list(Children)),
	(?ALTERNATIVE_BLOCK_EXPIRATION_TIME_SECONDS) * ForkLen.

get_fork_length(Tab, Branches) when is_list(Branches) ->
	1 + lists:max([0 | [get_fork_length(Tab, Branch) || Branch <- Branches]]);
get_fork_length(Tab, Branch) ->
	[{_, {_B, _Status, _Timestamp, Children}}] = ets:lookup(Tab, {block, Branch}),
	case sets:size(Children) == 0 of
		true ->
			1;
		false ->
			1 + get_fork_length(Tab, sets:to_list(Children))
	end.

%% @doc Update the status of the given block to 'nonce_limiter_validated'.
%% Do nothing if the block is not found in cache or if its status is
%% not 'awaiting_nonce_limiter_validation'.
mark_nonce_limiter_validated(Tab, H) ->
	case ets:lookup(Tab, {block, H}) of
		[{_, {B, {not_validated, awaiting_nonce_limiter_validation}, Timestamp, Children}}] ->
			insert(Tab, {{block, H}, {B,
					{not_validated, nonce_limiter_validated}, Timestamp, Children}});
		_ ->
			ok
	end.

%% @doc Add a validated block to the cache. If the block is already in the cache, it
%% is overwritten. However, the function assumes the height, hash, previous hash, and
%% the cumulative difficulty do not change.
%% Raises previous_block_not_found if the previous block is not in the cache.
%% Raises previous_block_not_validated if the previous block is not validated.
add_validated(Tab, B) ->
	#block{ indep_hash = H, hash = SolutionH, previous_block = PrevH, height = Height } = B,
	case ets:lookup(Tab, {block, PrevH}) of
		[] ->
			error(previous_block_not_found);
		[{_, {_PrevB, {not_validated, _}, _Timestamp, _Children}}] ->
			error(previous_block_not_validated);
		[{_, {PrevB, PrevStatus, PrevTimestamp, PrevChildren}}] ->
			case ets:lookup(Tab, {block, H}) of
				[] ->
					RemainingHs = remove_expired_alternative_blocks(Tab, SolutionH),
					SolutionSet = sets:from_list([H | RemainingHs]),
					[{_, Set}] = ets:lookup(Tab, links),
					Status = validated,
					insert(Tab, [
						{{block, PrevH}, {PrevB, PrevStatus, PrevTimestamp,
								sets:add_element(H, PrevChildren)}},
						{{block, H}, {B, Status, erlang:timestamp(), sets:new()}},
						{max_cdiff, maybe_increase_max_cdiff(Tab, B, Status)},
						{links, gb_sets:insert({Height, H}, Set)},
						{{solution, SolutionH}, SolutionSet}
					]);
				[{_, {_B, on_chain, Timestamp, Children}}] ->
					insert(Tab, [
						{{block, PrevH}, {PrevB, PrevStatus, PrevTimestamp,
								sets:add_element(H, PrevChildren)}},
						{{block, H}, {B, on_chain, Timestamp, Children}}
					]);
				[{_, {_B, _Status, Timestamp, Children}}] ->
					insert(Tab, [
						{{block, PrevH}, {PrevB, PrevStatus, PrevTimestamp,
								sets:add_element(H, PrevChildren)}},
						{{block, H}, {B, validated, Timestamp, Children}}
					])
			end
	end.

%% @doc Get the block from cache. Returns not_found if the block is not in cache.
get(Tab, H) ->
	case ets:lookup(Tab, {block, H}) of
		[] ->
			not_found;
		[{_, {B, _Status, _Timestamp, _Children}}] ->
			B
	end.

%% @doc Get the block and its status from cache.
%% Returns not_found if the block is not in cache.
get_block_and_status(Tab, H) ->
	case ets:lookup(Tab, {block, H}) of
		[] ->
			not_found;
		[{_, {B, Status, Timestamp, _Children}}] ->
			{B, {Status, Timestamp}}
	end.

%% @doc Get a {block, previous blocks, status} tuple for the earliest block from
%% the longest chain, which has not been validated yet. The previous blocks are
%% sorted from newest to oldest. The last one is a block from the current fork.
%% status is a tuple that indicates where in the validation process block is.
get_earliest_not_validated_from_longest_chain(Tab) ->
	[{_, Tip}] = ets:lookup(Tab, tip),
	[{_, {CDiff, H}}] = ets:lookup(Tab, max_cdiff),
	[{_, {#block{ cumulative_diff = TipCDiff }, _, _, _}}] = ets:lookup(Tab, {block, Tip}),
	case TipCDiff >= CDiff of
		true ->
			%% Current Tip is tip of the longest chain
			not_found;
		false ->
			[{_, {B, Status, Timestamp, _Children}}] = ets:lookup(Tab, {block, H}),
			case Status of
				{not_validated, _} ->
					get_earliest_not_validated(Tab, B, Status, Timestamp);
				_ ->
					not_found
			end
	end.

%% @doc Return the list of {BH, TXIDs} pairs corresponding to the top up to the
%% ?STORE_BLOCKS_BEHIND_CURRENT blocks of the longest chain and the number of blocks
%% in this list that are not on chain yet.
%%
%% The cache is updated via update_longest_chain_cache/1 which calls
%% get_longest_chain_block_txs_pairs/7
get_longest_chain_cache(Tab) ->
	[{longest_chain, LongestChain}] = ets:lookup(Tab, longest_chain),
	LongestChain.

get_longest_chain_block_txs_pairs(_Tab, _H, 0, _PrevStatus, _PrevH, Pairs, NotOnChainCount) ->
	{lists:reverse(Pairs), NotOnChainCount};
get_longest_chain_block_txs_pairs(Tab, H, N, PrevStatus, PrevH, Pairs, NotOnChainCount) ->
	case ets:lookup(Tab, {block, H}) of
		[{_, {B, {not_validated, awaiting_nonce_limiter_validation}, _Timestamp,
				_Children}}] ->
			get_longest_chain_block_txs_pairs(Tab, B#block.previous_block,
					?STORE_BLOCKS_BEHIND_CURRENT, none, none, [], 0);
		[{_, {B, Status, _Timestamp, _Children}}] ->
			case PrevStatus == on_chain andalso Status /= on_chain of
				true ->
					%% A reorg should have happened in the meantime - an unlikely
					%% event, retry.
					get_longest_chain_cache(Tab);
				false ->
					NotOnChainCount2 =
						case Status of
							on_chain ->
								NotOnChainCount;
							_ ->
								NotOnChainCount + 1
						end,
					Pairs2 = [{B#block.indep_hash, [tx_id(TX) || TX <- B#block.txs]} | Pairs],
					get_longest_chain_block_txs_pairs(Tab, B#block.previous_block, N - 1,
							Status, H, Pairs2, NotOnChainCount2)
			end;
		[] ->
			case PrevStatus of
				on_chain ->
					case ets:lookup(Tab, {block, PrevH}) of
						[] ->
							%% The block has been pruned -
							%% an unlikely race condition so we retry.
							get_longest_chain_cache(Tab);
						[_] ->
							%% Pairs already contains the deepest block of the cache.
							{lists:reverse(Pairs), NotOnChainCount}
					end;
				_ ->
					%% The block has been invalidated -
					%% an unlikely race condition so we retry.
					get_longest_chain_cache(Tab)
			end
	end.

tx_id(#tx{ id = ID }) ->
	ID;
tx_id(TXID) ->
	TXID.

%% @doc Mark the given block as the tip block. Mark the previous blocks as on-chain.
%% Mark the on-chain blocks from other forks as validated. Raises invalid_tip if
%% one of the preceeding blocks is not validated. Raises not_found if the block
%% is not found.
%%
%% Setting a new tip can cause some branches to be invalidated by the checkpoint, so we need
%% to recalculate max_cdiff.
mark_tip(Tab, H) ->
	case ets:lookup(Tab, {block, H}) of
		[{_, {B, Status, Timestamp, Children}}] ->
			case is_valid_fork(Tab, B, Status) of
				true ->
					insert(Tab, [
						{tip, H},
						{{block, H}, {B, on_chain, Timestamp, Children}} |
						mark_on_chain(Tab, B)
					]),
					%% We would only update max_cdiff if somehow the old max_cdiff was on a branch
					%% that has been invalidated due to the new tip causing the checkpoint to move.
					%% In practice we would not expect this to happen.
					[{_, {_CDiff, CDiffH}}] = ets:lookup(Tab, max_cdiff),
					case is_valid_fork(Tab, CDiffH) of
						true ->
							ok;
						false ->
							insert(Tab, {max_cdiff, find_max_cdiff(Tab, B#block.height)})
					end;
				false ->
					error(invalid_tip)
			end;
		[] ->
			error(not_found)
	end.

%% @doc Remove the block and all the blocks on top from the cache.
remove(Tab, H) ->
	case ets:lookup(Tab, {block, H}) of
		[] ->
			ok;
		[{_, {#block{ previous_block = PrevH }, _Status, _Timestamp, _Children}}] ->
			[{_, C = {_, H2}}] = ets:lookup(Tab, max_cdiff),
			[{_, {PrevB, PrevBStatus, PrevTimestamp, PrevBChildren}}] = ets:lookup(Tab,
					{block, PrevH}),
			remove2(Tab, H),
			insert(Tab, [
				{max_cdiff, case ets:lookup(Tab, {block, H2}) of
								[] ->
									find_max_cdiff(Tab, get_tip_height(Tab));
								_ ->
									C
							end},
				{{block, PrevH}, {PrevB, PrevBStatus, PrevTimestamp,
						sets:del_element(H, PrevBChildren)}}
			]),
			ar_ignore_registry:remove(H),
			ok
	end.

get_checkpoint_block(RecentBI) ->
	get_checkpoint_block2(RecentBI, 1, ?CHECKPOINT_DEPTH).

%% @doc Prune the cache.  Discard all blocks deeper than Depth from the tip and
%% all of their children that are not on_chain.
%% 
%% Height 99              A    B' C
%%                         \  /   |
%%        98                D'    E
%%						      \  /
%%        97                   F' 
%%
%% B' is the Tip. prune(Tab, 1) will remove F', E, and C from the cache.             
prune(Tab, Depth) ->
	prune2(Tab, Depth, get_tip_height(Tab)).

%% @doc Return true if there is at least one block in the cache with the given solution hash.
is_known_solution_hash(Tab, SolutionH) ->
	case ets:lookup(Tab, {solution, SolutionH}) of
		[] ->
			false;
		[{_, _Set}] ->
			true
	end.

%% @doc Return a block from the block cache meeting the following requirements:
%% - hash == SolutionH;
%% - indep_hash /= H.
%%
%% If there are several blocks, choose one with the same cumulative difficulty
%% or CDiff > PrevCDiff2 and CDiff2 > PrevCDiff (double-signing). If there are no
%% such blocks, return any other block matching the conditions above. Return not_found
%% if there are no blocks matching those conditions.
get_by_solution_hash(Tab, SolutionH, H, CDiff, PrevCDiff) ->
	case ets:lookup(Tab, {solution, SolutionH}) of
		[] ->
			not_found;
		[{_, Set}] ->
			get_by_solution_hash(Tab, SolutionH, H, CDiff, PrevCDiff, sets:to_list(Set), none)
	end.

get_by_solution_hash(_Tab, _SolutionH, _H, _CDiff, _PrevCDiff, [], B) ->
	case B of
		none ->
			not_found;
		_ ->
			B
	end;
get_by_solution_hash(Tab, SolutionH, H, CDiff, PrevCDiff, [H | L], B) ->
	get_by_solution_hash(Tab, SolutionH, H, CDiff, PrevCDiff, L, B);
get_by_solution_hash(Tab, SolutionH, H, CDiff, PrevCDiff, [H2 | L], _B) ->
	case get(Tab, H2) of
		not_found ->
			%% An extremely unlikely race condition - simply retry.
			get_by_solution_hash(Tab, SolutionH, H, CDiff, PrevCDiff);
		#block{ cumulative_diff = CDiff } = B2 ->
			B2;
		#block{ cumulative_diff = CDiff2, previous_cumulative_diff = PrevCDiff2 } = B2
				when CDiff2 > PrevCDiff, CDiff > PrevCDiff2 ->
			B2;
		B2 ->
			get_by_solution_hash(Tab, SolutionH, H, CDiff, PrevCDiff, L, B2)
	end.

%% @doc Return the list of siblings of the given block, if any.
get_siblings(Tab, B) ->
	H = B#block.indep_hash,
	PrevH = B#block.previous_block,
	case ets:lookup(Tab, {block, PrevH}) of
		[] ->
			[];
		[{_, {_B, _Status, _CurrentTimestamp, Children}}] ->
			sets:fold(
				fun(SibH, Acc) ->
					case SibH of
						H ->
							Acc;
						_ ->
							case ets:lookup(Tab, {block, SibH}) of
								[] ->
									Acc;
								[{_, {Sib, _, _, _}}] ->
									[Sib | Acc]
							end
					end
				end,
				[],
				Children
			)
	end.

update_timestamp(Tab, H, ReceiveTimestamp) ->
	case ets:lookup(Tab, {block, H}) of
		[] ->
			not_found;
		[{_, {B, Status, Timestamp, Children}}] ->
			case B#block.receive_timestamp of
				undefined ->
					insert(Tab, {
							{block, H},
							{
								B#block{receive_timestamp = ReceiveTimestamp},
								Status,
								Timestamp,
								Children
							}
						}, false);
				_ ->
					ok
			end
	end.

%%%===================================================================
%%% Private functions.
%%%===================================================================

insert(Tab, Args) ->
	insert(Tab, Args, true).
insert(Tab, Args, UpdateCache) ->
	ets:insert(Tab, Args),
	case UpdateCache of
		true ->
			update_longest_chain_cache(Tab);
		false ->
			ok
	end.

delete(Tab, Args) ->
	delete(Tab, Args, true).
delete(Tab, Args, UpdateCache) ->
	ets:delete(Tab, Args),
	case UpdateCache of
		true ->
			update_longest_chain_cache(Tab);
		false ->
			ok
	end.

get_earliest_not_validated(Tab, #block{ previous_block = PrevH } = B, Status, Timestamp) ->
	[{_, {PrevB, PrevStatus, PrevTimestamp, _Children}}] = ets:lookup(Tab, {block, PrevH}),
	case PrevStatus of
		{not_validated, _} ->
			get_earliest_not_validated(Tab, PrevB, PrevStatus, PrevTimestamp);
		_ ->
			{B, get_fork_blocks(Tab, B), {Status, Timestamp}}
	end.

get_fork_blocks(Tab, #block{ previous_block = PrevH }) ->
	[{_, {PrevB, Status, _Timestamp, _Children}}] = ets:lookup(Tab, {block, PrevH}),
	case Status of
		on_chain ->
			[PrevB];
		_ ->
			[PrevB | get_fork_blocks(Tab, PrevB)]
	end.

mark_on_chain(Tab, #block{ previous_block = PrevH, indep_hash = H }) ->
	case ets:lookup(Tab, {block, PrevH}) of
		[{_, {_PrevB, {not_validated, _}, _Timestamp, _Children}}] ->
			error(invalid_tip);
		[{_, {_PrevB, on_chain, _Timestamp, Children}}] ->
			%% Mark the blocks from the previous main fork as validated, not on-chain.
			mark_off_chain(Tab, sets:del_element(H, Children));
		[{_, {PrevB, validated, Timestamp, Children}}] ->
			[{{block, PrevH}, {PrevB, on_chain, Timestamp, Children}}
					| mark_on_chain(Tab, PrevB)]
	end.

mark_off_chain(Tab, Set) ->
	sets:fold(
		fun(H, Acc) ->
			case ets:lookup(Tab, {block, H}) of
				[{_, {B, on_chain, Timestamp, Children}}] ->
					[{{block, H}, {B, validated, Timestamp, Children}}
							| mark_off_chain(Tab, Children)];
				_ ->
					Acc
			end
		end,
		[],
		Set
	).

remove2(Tab, H) ->
	[{_, Set}] = ets:lookup(Tab, links),
	case ets:lookup(Tab, {block, H}) of
		not_found ->
			ok;
		[{_, {#block{ hash = SolutionH, height = Height }, _Status, _Timestamp, Children}}] ->
			%% Don't update the cache here. remove/2 will do it.
			delete(Tab, {block, H}, false), 
			ar_ignore_registry:remove(H),
			remove_solution(Tab, H, SolutionH),
			insert(Tab, {links, gb_sets:del_element({Height, H}, Set)}, false),
			sets:fold(
				fun(Child, ok) ->
					remove2(Tab, Child)
				end,
				ok,
				Children
			)
	end.

remove_solution(Tab, H, SolutionH) ->
	[{_, SolutionSet}] = ets:lookup(Tab, {solution, SolutionH}),
	case sets:size(SolutionSet) of
		1 ->
			delete(Tab, {solution, SolutionH}, false);
		_ ->
			SolutionSet2 = sets:del_element(H, SolutionSet),
			insert(Tab, {{solution, SolutionH}, SolutionSet2}, false)
	end.

get_tip_height(Tab) ->
	[{_, Tip}] = ets:lookup(Tab, tip),
	[{_, {#block{ height = Height }, _Status, _Timestamp, _Children}}] = ets:lookup(Tab,
			{block, Tip}),
	Height.

get_checkpoint_height(TipHeight) ->
	TipHeight - ?CHECKPOINT_DEPTH + 1.

get_checkpoint_block2([{H, _, _}], _N, _CheckpointDepth) ->
	%% The genesis block.
	ar_block_cache:get(block_cache, H);
get_checkpoint_block2([{H, _, _} | BI], N, CheckpointDepth) ->
	 B = ar_block_cache:get(block_cache, H),
	 get_checkpoint_block2(BI, N + 1, B, CheckpointDepth).

get_checkpoint_block2([{H, _, _}], _N, B, _CheckpointDepth) ->
	%% The genesis block.
	case ar_block_cache:get(block_cache, H) of
		not_found ->
			B;
		B2 ->
			B2
	end;
get_checkpoint_block2([{H, _, _} | _], N, B, CheckpointDepth) when N == CheckpointDepth ->
	case ar_block_cache:get(block_cache, H) of
		not_found ->
			B;
		B2 ->
			B2
	end;
get_checkpoint_block2([{H, _, _} | BI], N, B, CheckpointDepth) ->
	case ar_block_cache:get(block_cache, H) of
		not_found ->
			B;
		B2 ->
			get_checkpoint_block2(BI, N + 1, B2, CheckpointDepth)
	end.

%% @doc Return true if B is either on the main fork or on a fork which branched off at the
%% checkpoint height or higher.
is_valid_fork(Tab, H) ->
	[{_, {B, Status, _Timestamp, _Children}}] = ets:lookup(Tab, {block, H}),
	is_valid_fork(Tab, B, Status).
is_valid_fork(Tab, B, Status) ->
	CheckpointHeight = get_checkpoint_height(get_tip_height(Tab)),
	is_valid_fork(Tab, B, Status, CheckpointHeight).

is_valid_fork(_Tab, #block{ height = Height, indep_hash = H }, _Status, CheckpointHeight)
  		when Height < CheckpointHeight ->
	?LOG_WARNING([{event, found_invalid_heavy_fork}, {hash, ar_util:encode(H)},
				{height, Height}, {checkpoint_height, CheckpointHeight}]),
	false;
is_valid_fork(_Tab, _B, on_chain, _CheckpointHeight) ->
	true;
is_valid_fork(Tab, B, _Status, CheckpointHeight) ->
	[{_, {PrevB, PrevStatus, _, _}}] = ets:lookup(Tab, {block, B#block.previous_block}),
	is_valid_fork(Tab, PrevB, PrevStatus, CheckpointHeight).

maybe_increase_max_cdiff(Tab, B, Status) ->
	[{_, C}] = ets:lookup(Tab, max_cdiff),
	maybe_increase_max_cdiff(Tab, B, Status, C).

maybe_increase_max_cdiff(Tab, B, Status, {MaxCDiff, _H} = C) ->
	case B#block.cumulative_diff > MaxCDiff andalso is_valid_fork(Tab, B, Status) of
		true ->
			{B#block.cumulative_diff, B#block.indep_hash};
		false ->
			C
	end.

find_max_cdiff(Tab, TipHeight) ->
	CheckpointHeight = get_checkpoint_height(TipHeight),
	[{_, Set}] = ets:lookup(Tab, links),
	gb_sets:fold(
		fun ({Height, _H}, Acc) when Height < CheckpointHeight ->
				Acc;
			({_Height, H}, not_set) ->
				[{_, {#block{ cumulative_diff = CDiff }, _, _, _}}] = ets:lookup(Tab,
						{block, H}),
				{CDiff, H};
			({_Height, H}, Acc) ->
				[{_, {B, Status, _, _}}] = ets:lookup(Tab, {block, H}),
				maybe_increase_max_cdiff(Tab, B, Status, Acc)
		end,
		not_set,
		Set
	).

prune2(Tab, Depth, TipHeight) ->
	[{_, Set}] = ets:lookup(Tab, links),
	case gb_sets:is_empty(Set) of
		true ->
			ok;
		false ->
			{{Height, H}, Set2} = gb_sets:take_smallest(Set),
			case Height >= TipHeight - Depth of
				true ->
					ok;
				false ->
					insert(Tab, {links, Set2}, false),
					%% The lowest block must be on-chain by construction.
					[{_, {B, on_chain, _Timestamp, Children}}] = ets:lookup(Tab, {block, H}),
					#block{ hash = SolutionH } = B,
					sets:fold(
						fun(Child, ok) ->
							[{_, {_, Status, _, _}}] = ets:lookup(Tab, {block, Child}),
							case Status of
								on_chain ->
									ok;
								_ ->
									remove(Tab, Child)
							end
						end,
						ok,
						Children
					),
					remove_solution(Tab, H, SolutionH),
					delete(Tab, {block, H}),
					ar_ignore_registry:remove(H),
					prune2(Tab, Depth, TipHeight)
			end
	end.

update_longest_chain_cache(Tab) ->
	[{_, {_CDiff, H}}] = ets:lookup(Tab, max_cdiff),
	Result = get_longest_chain_block_txs_pairs(Tab, H, ?STORE_BLOCKS_BEHIND_CURRENT,
			none, none, [], 0),
	case ets:update_element(Tab, longest_chain, {2, Result}) of
		true -> ok;
		false ->
			%% if insert_new fails it means another process added the longest_chain key
			%% between when we called update_element here. Extremely unlikely, really only
			%% possible when the node first starts up, and ultimately not super relevant since
			%% the cache will likely be refreshed again shortly. So we'll ignore.
			ets:insert_new(Tab, {longest_chain, Result})
	end,
	Result.

%%%===================================================================
%%% Tests.
%%%===================================================================

checkpoint_test() ->
	ets:new(bcache_test, [set, named_table]),

	%% Height	Block/Status
	%%
	%% 3		B3/on_chain
	%%			     | 
	%% 2		B2/on_chain        B2B/not_validated
	%%			     |                  |
	%% 1		B1/on_chain        B1B/not_validated
	%%					  \            /
	%% 0					B0/on_chain
	new(bcache_test, B0 = random_block(0)),
	add(bcache_test, B1 = on_top(random_block(1), B0)),
	add(bcache_test, B1B = on_top(random_block(1), B0)),
	add(bcache_test, B2 = on_top(random_block(2), B1)),
	add(bcache_test, B2B = on_top(random_block(2), B1B)),
	add(bcache_test, B3 = on_top(random_block(3), B2)),
	mark_tip(bcache_test, block_id(B1)),
	mark_tip(bcache_test, block_id(B2)),
	mark_tip(bcache_test, block_id(B3)),

	?assertMatch(not_found, get_earliest_not_validated_from_longest_chain(bcache_test)),
	assert_longest_chain([B3, B2, B1, B0], 0),
	assert_tip(block_id(B3)),
	assert_max_cdiff({3, block_id(B3)}),
	assert_is_valid_fork(true, on_chain, B0),
	assert_is_valid_fork(true, on_chain, B1),
	assert_is_valid_fork(true, not_validated, B1B),
	assert_is_valid_fork(true, on_chain, B2),
	assert_is_valid_fork(true, not_validated, B2B),
	assert_is_valid_fork(true, on_chain, B3),

	%% Add B4 as not_validated. No blocks are pushed beneath the checkpoint height since B4
	%% has not been validated.
	%%
	%% Height	Block/Status
	%%
	%% 4		B4/not_validated
	%%			     |
	%% 3		B3/on_chain
	%%			     | 
	%% 2		B2/on_chain        B2B/not_validated/invalid_fork
	%%			     |                  |
	%% 1		B1/on_chain        B1B/not_validated/invalid_fork
	%%					  \            /
	%% 0					B0/on_chain/invalid_fork
	add(bcache_test, B4 = on_top(random_block(4), B3)),

	?assertMatch({B4, [B3], {{not_validated, awaiting_nonce_limiter_validation}, _}},
		get_earliest_not_validated_from_longest_chain(bcache_test)),
	assert_longest_chain([B3, B2, B1, B0], 0),
	assert_tip(block_id(B3)),
	assert_max_cdiff({4, block_id(B4)}),
	assert_is_valid_fork(true, on_chain, B0),
	assert_is_valid_fork(true, on_chain, B1),
	assert_is_valid_fork(true, not_validated, B1B),
	assert_is_valid_fork(true, on_chain, B2),
	assert_is_valid_fork(true, not_validated, B2B),
	assert_is_valid_fork(true, on_chain, B3),
	assert_is_valid_fork(true, not_validated, B4),

	%% Mark B4 as the tip, this pushes B0 below the checkpoint height and invalidates B1B and B2B.
	%%
	%% Height	Block/Status
	%%
	%% 4		B4/on_chain
	%%			     |
	%% 3		B3/on_chain
	%%			     | 
	%% 2		B2/on_chain        B2B/not_validated/invalid_fork
	%%			     |                  |
	%% 1		B1/on_chain        B1B/not_validated/invalid_fork
	%%					  \            /
	%% 0					B0/on_chain/invalid_fork
	mark_tip(bcache_test, block_id(B4)),

	?assertMatch(not_found, get_earliest_not_validated_from_longest_chain(bcache_test)),
	assert_longest_chain([B4, B3, B2, B1, B0], 0),
	assert_tip(block_id(B4)),
	assert_max_cdiff({4, block_id(B4)}),
	assert_is_valid_fork(false, on_chain, B0),
	assert_is_valid_fork(true, on_chain, B1),
	assert_is_valid_fork(false, not_validated, B1B),
	assert_is_valid_fork(true, on_chain, B2),
	assert_is_valid_fork(false, not_validated, B2B),
	assert_is_valid_fork(true, on_chain, B3),
	assert_is_valid_fork(true, on_chain, B4),

	%% Add B3B with cdiff 5 to the invalid fork. Nothing should change.
	%%
	%% Height	Block/Status
	%%
	%% 4		B4/on_chain
	%%			     |
	%% 3		B3/on_chain        B3B/not_validated/invalid_fork
	%%			     |                  |
	%% 2		B2/on_chain        B2B/not_validated/invalid_fork
	%%			     |                  |
	%% 1		B1/on_chain        B1B/not_validated/invalid_fork
	%%					  \            /
	%% 0					B0/on_chain/invalid_fork
	add(bcache_test, B3B = on_top(random_block(5), B2B)),

	?assertMatch(not_found, get_earliest_not_validated_from_longest_chain(bcache_test)),
	assert_longest_chain([B4, B3, B2, B1, B0], 0),
	assert_tip(block_id(B4)),
	assert_max_cdiff({4, block_id(B4)}),
	assert_is_valid_fork(false, on_chain, B0),
	assert_is_valid_fork(true, on_chain, B1),
	assert_is_valid_fork(false, not_validated, B1B),
	assert_is_valid_fork(true, on_chain, B2),
	assert_is_valid_fork(false, not_validated, B2B),
	assert_is_valid_fork(true, on_chain, B3),
	assert_is_valid_fork(true, on_chain, B4),
	assert_is_valid_fork(false, not_validated, B3B),
	
	%% Remove B4, this should revalidate the fork and make it the max_cdiff
	%%
	%% Height	Block/Status
	%%
	%% 3		B3/on_chain        B3B/not_validated
	%%			     |                  |
	%% 2		B2/on_chain        B2B/not_validated
	%%			     |                  |
	%% 1		B1/on_chain        B1B/not_validated
	%%					  \            /
	%% 0					B0/on_chain
	mark_tip(bcache_test, block_id(B3)),
	remove(bcache_test, block_id(B4)),
	
	?assertMatch({B1B, [B0], {{not_validated, awaiting_nonce_limiter_validation}, _}},
		get_earliest_not_validated_from_longest_chain(bcache_test)),
	assert_longest_chain([B0], 0),
	assert_tip(block_id(B3)),
	assert_max_cdiff({5, block_id(B3B)}),
	assert_is_valid_fork(true, on_chain, B0),
	assert_is_valid_fork(true, on_chain, B1),
	assert_is_valid_fork(true, not_validated, B1B),
	assert_is_valid_fork(true, on_chain, B2),
	assert_is_valid_fork(true, not_validated, B2B),
	assert_is_valid_fork(true, on_chain, B3),
	assert_is_valid_fork(true, not_validated, B3B),

	%% We should not be able to mark blocks on the invalid fork as the tip.
	%%
	%% Height	Block/Status
	%%
	%% 4		B4/on_chain
	%%			     |
	%% 3		B3/on_chain        B3B/not_validated/invalid_fork
	%%			     |                  |
	%% 2		B2/on_chain        B2B/not_validated/invalid_fork
	%%			     |                  |
	%% 1		B1/on_chain        B1B/not_validated/invalid_fork
	%%					  \            /
	%% 0					B0/on_chain/invalid_fork
	add(bcache_test, B4),
	mark_tip(bcache_test, block_id(B4)),

	?assertException(error, invalid_tip, mark_tip(bcache_test, block_id(B1B))),
	?assertMatch(not_found, get_earliest_not_validated_from_longest_chain(bcache_test)),
	assert_longest_chain([B4, B3, B2, B1, B0], 0),
	assert_tip(block_id(B4)),
	assert_max_cdiff({4, block_id(B4)}),
	assert_is_valid_fork(false, on_chain, B0),
	assert_is_valid_fork(true, on_chain, B1),
	assert_is_valid_fork(false, not_validated, B1B),
	assert_is_valid_fork(true, on_chain, B2),
	assert_is_valid_fork(false, not_validated, B2B),
	assert_is_valid_fork(true, on_chain, B3),
	assert_is_valid_fork(false, not_validated, B3B),

	ets:delete(bcache_test).

checkpoint_invalidate_max_cdiff_test() ->
	ets:new(bcache_test, [set, named_table]),

	%% B2B is heaviest.
	%%
	%% Height	Block/Status
	%%
	%% 4		B4/not_validated
	%%			     |
	%% 3		B3/on_chain
	%%			     | 
	%% 2		B2/on_chain        B2B/not_validated
	%%			     |                  |
	%% 1		B1/on_chain        B1B/not_validated
	%%					  \            /
	%% 0					B0/on_chain
	new(bcache_test, B0 = random_block(0)),
	add(bcache_test, B1 = on_top(random_block(1), B0)),
	add(bcache_test, B2 = on_top(random_block(2), B1)),
	add(bcache_test, B3 = on_top(random_block(3), B2)),
	add(bcache_test, B4 = on_top(random_block(4), B3)),
	add(bcache_test, B1B = on_top(random_block(1), B0)),
	add(bcache_test, B2B = on_top(random_block(5), B1B)),	
	mark_tip(bcache_test, block_id(B1)),
	mark_tip(bcache_test, block_id(B2)),
	mark_tip(bcache_test, block_id(B3)),

	?assertMatch({B1B, [B0], {{not_validated, awaiting_nonce_limiter_validation}, _}},
		get_earliest_not_validated_from_longest_chain(bcache_test)),
	assert_longest_chain([B0], 0),
	assert_tip(block_id(B3)),
	assert_max_cdiff({5, block_id(B2B)}),
	assert_is_valid_fork(true, on_chain, B0),
	assert_is_valid_fork(true, on_chain, B1),
	assert_is_valid_fork(true, not_validated, B1B),
	assert_is_valid_fork(true, on_chain, B2),
	assert_is_valid_fork(true, not_validated, B2B),
	assert_is_valid_fork(true, on_chain, B3),
	assert_is_valid_fork(true, not_validated, B4),

	%% B2B is still heaviest, but since B4 is the new tip, B2B's branch has been pushed below
	%% the checkpoint.
	%%
	%% Height	Block/Status
	%%
	%% 4		B4/on_chain
	%%			     |
	%% 3		B3/on_chain
	%%			     | 
	%% 2		B2/on_chain        B2B/not_validated/invalid_fork
	%%			     |                  |
	%% 1		B1/on_chain        B1B/not_validated/invalid_fork
	%%					  \            /
	%% 0					B0/on_chain/invalid_fork
	mark_tip(bcache_test, block_id(B4)),

	?assertMatch(not_found, get_earliest_not_validated_from_longest_chain(bcache_test)),
	assert_longest_chain([B4, B3, B2, B1, B0], 0),
	assert_tip(block_id(B4)),
	assert_max_cdiff({4, block_id(B4)}),
	assert_is_valid_fork(false, on_chain, B0),
	assert_is_valid_fork(true, on_chain, B1),
	assert_is_valid_fork(false, not_validated, B1B),
	assert_is_valid_fork(true, on_chain, B2),
	assert_is_valid_fork(false, not_validated, B2B),
	assert_is_valid_fork(true, on_chain, B3),
	assert_is_valid_fork(true, on_chain, B4),

	ets:delete(bcache_test).

checkpoint_invalidate_tip_test() ->
	ets:new(bcache_test, [set, named_table]),

	%% B2B is the tip
	%%
	%% Height	Block/Status
	%%
	%% 4		B4/not_validated
	%%			     |
	%% 3		B3/validated
	%%			     | 
	%% 2		B2/validated       B2B/on_chain
	%%			     |                  |
	%% 1		B1/validated       B1B/on_chain
	%%					  \            /
	%% 0					B0/on_chain
	new(bcache_test, B0 = random_block(0)),
	add(bcache_test, B1 = on_top(random_block(1), B0)),
	add(bcache_test, B2 = on_top(random_block(2), B1)),
	add(bcache_test, B3 = on_top(random_block(3), B2)),
	add(bcache_test, B4 = on_top(random_block(4), B3)),
	add(bcache_test, B1B = on_top(random_block(1), B0)),
	add(bcache_test, B2B = on_top(random_block(5), B1B)),
	mark_tip(bcache_test, block_id(B1)),
	mark_tip(bcache_test, block_id(B2)),
	mark_tip(bcache_test, block_id(B3)),
	mark_tip(bcache_test, block_id(B1B)),
	mark_tip(bcache_test, block_id(B2B)),

	?assertMatch(not_found,
		get_earliest_not_validated_from_longest_chain(bcache_test)),
	assert_longest_chain([B2B, B1B, B0], 0),
	assert_tip(block_id(B2B)),
	assert_max_cdiff({5, block_id(B2B)}),
	assert_is_valid_fork(true, on_chain, B0),
	assert_is_valid_fork(true, validated, B1),
	assert_is_valid_fork(true, on_chain, B1B),
	assert_is_valid_fork(true, validated, B2),
	assert_is_valid_fork(true, on_chain, B2B),
	assert_is_valid_fork(true, validated, B3),
	assert_is_valid_fork(true, not_validated, B4),

	%% When we mark B4 as the tip it will also invalidate the B2B branch.
	%%
	%% Height	Block/Status
	%%
	%% 4		B4/on_chain
	%%			     |
	%% 3		B3/on_chain
	%%			     | 
	%% 2		B2/on_chain        B2B/not_validated/invalid_fork
	%%			     |                  |
	%% 1		B1/on_chain        B1B/not_validated/invalid_fork
	%%					  \            /
	%% 0					B0/on_chain/invalid_fork
	mark_tip(bcache_test, block_id(B4)),

	?assertMatch(not_found, get_earliest_not_validated_from_longest_chain(bcache_test)),
	assert_longest_chain([B4, B3, B2, B1, B0], 0),
	assert_tip(block_id(B4)),
	assert_max_cdiff({4, block_id(B4)}),
	assert_is_valid_fork(false, on_chain, B0),
	assert_is_valid_fork(true, on_chain, B1),
	assert_is_valid_fork(false, validated, B1B),
	assert_is_valid_fork(true, on_chain, B2),
	assert_is_valid_fork(false, validated, B2B),
	assert_is_valid_fork(true, on_chain, B3),
	assert_is_valid_fork(true, on_chain, B4),

	ets:delete(bcache_test).

block_cache_test() ->
	ets:new(bcache_test, [set, named_table]),

	%% Initialize block_cache from B1
	%%
	%% Height		Block/Status
	%%
	%% 0			B1/on_chain
	new(bcache_test, B1 = random_block(0)),
	?assertEqual(not_found, get(bcache_test, crypto:strong_rand_bytes(48))),
	?assertEqual(not_found, get_by_solution_hash(bcache_test, crypto:strong_rand_bytes(32),
			crypto:strong_rand_bytes(32), 1, 1)),
	?assertEqual(B1, get(bcache_test, block_id(B1))),
	?assertEqual(B1, get_by_solution_hash(bcache_test, B1#block.hash,
			crypto:strong_rand_bytes(32), 1, 1)),
	?assertEqual(not_found, get_by_solution_hash(bcache_test, B1#block.hash,
			block_id(B1), 1, 1)),
	assert_longest_chain([B1], 0),
	assert_tip(block_id(B1)),
	assert_max_cdiff({0, block_id(B1)}),
	assert_is_valid_fork(true, on_chain, B1),
	?assertEqual([], get_siblings(bcache_test, B1)),

	%% Re-adding B1 shouldn't change anything - i.e. nothing should be updated because the
	%% block is already on chain
	%%
	%% Height		Block/Status
	%%
	%% 0			B1/on_chain
	add(bcache_test, B1#block{ txs = [crypto:strong_rand_bytes(32)] }),
	?assertEqual(B1#block{ txs = [] }, get(bcache_test, block_id(B1))),
	?assertEqual(B1#block{ txs = [] }, get_by_solution_hash(bcache_test, B1#block.hash,
			crypto:strong_rand_bytes(32), 1, 1)),
	assert_longest_chain([B1], 0),
	assert_max_cdiff({0, block_id(B1)}),
	assert_is_valid_fork(true, on_chain, B1),

	%% Same as above.
	%%
	%% Height		Block/Status
	%%
	%% 0			B1/on_chain
	add(bcache_test, B1),
	?assertEqual(not_found, get_earliest_not_validated_from_longest_chain(bcache_test)),
	assert_longest_chain([B1], 0),
	assert_tip(block_id(B1)),
	assert_max_cdiff({0, block_id(B1)}),
	assert_is_valid_fork(true, on_chain, B1),

	%% Add B2 as not_validated
	%%
	%% Height	Block/Status
	%%
	%% 1		B2/not_validated
	%%				|
	%% 0		B1/on_chain
	add(bcache_test, B2 = on_top(random_block(1), B1)),
	ExpectedStatus = awaiting_nonce_limiter_validation,
	?assertMatch({B2, [B1], {{not_validated, ExpectedStatus}, _}},
			get_earliest_not_validated_from_longest_chain(bcache_test)),
	assert_longest_chain([B1], 0),
	assert_tip(block_id(B1)),
	assert_max_cdiff({1, block_id(B2)}),
	assert_is_valid_fork(true, on_chain, B1),
	assert_is_valid_fork(true, not_validated, B2),
	?assertEqual([], get_siblings(bcache_test, B2)),

	%% Add a TXID to B2, but still don't mark as validated
	%%
	%% Height	Block/Status
	%%
	%% 1		B2/not_validated
	%%				|
	%% 0		B1/on_chain
	TXID = crypto:strong_rand_bytes(32),
	add(bcache_test, B2#block{ txs = [TXID] }),
	?assertEqual(B2#block{ txs = [TXID] }, get(bcache_test, block_id(B2))),
	?assertEqual(B2#block{ txs = [TXID] }, get_by_solution_hash(bcache_test, B2#block.hash,
			crypto:strong_rand_bytes(32), 1, 1)),
	?assertEqual(B2#block{ txs = [TXID] }, get_by_solution_hash(bcache_test, B2#block.hash,
			block_id(B1), 1, 1)),
	assert_longest_chain([B1], 0),
	assert_tip(block_id(B1)),
	assert_max_cdiff({1, block_id(B2)}),
	assert_is_valid_fork(true, on_chain, B1),
	assert_is_valid_fork(true, not_validated, B2),

	%% Remove B2
	%%
	%% Height	Block/Status
	%%
	%% 0		B1/on_chain
	remove(bcache_test, block_id(B2)),
	?assertEqual(not_found, get(bcache_test, block_id(B2))),
	?assertEqual(B1, get(bcache_test, block_id(B1))),
	assert_longest_chain([B1], 0),
	assert_tip(block_id(B1)),
	assert_max_cdiff({0, block_id(B1)}),
	assert_is_valid_fork(true, on_chain, B1),

	%% Add B and B1_2 creating a fork, with B1_2 at a higher difficulty. Nether are validated.
	%%
	%% Height	Block/Status
	%%
	%% 1		B2/not_validated    B1_2/not_validated
	%%					  \             /
	%% 0					B1/on_chain
	add(bcache_test, B2),
	add(bcache_test, B1_2 = (on_top(random_block(2), B1))#block{ hash = B1#block.hash }),
	?assertEqual(B1, get_by_solution_hash(bcache_test, B1#block.hash, block_id(B1_2),
			1, 1)),
	?assertEqual(B1, get_by_solution_hash(bcache_test, B1#block.hash,
			crypto:strong_rand_bytes(32), B1#block.cumulative_diff, 1)),
	?assertEqual(B1_2, get_by_solution_hash(bcache_test, B1#block.hash,
			crypto:strong_rand_bytes(32), B1_2#block.cumulative_diff, 1)),
	?assert(lists:member(get_by_solution_hash(bcache_test, B1#block.hash, <<>>, 1, 1),
			[B1, B1_2])),
	assert_longest_chain([B1], 0),
	assert_tip(block_id(B1)),
	assert_max_cdiff({2, block_id(B1_2)}),
	assert_is_valid_fork(true, on_chain, B1),
	assert_is_valid_fork(true, not_validated, B2),
	assert_is_valid_fork(true, not_validated, B1_2),
	?assertEqual([B2], get_siblings(bcache_test, B1_2)),
	?assertEqual([B1_2], get_siblings(bcache_test, B2)),

	%% Even though B2 is marked as a tip, it is still lower difficulty than B1_2 so will
	%% not be included in the longest chain
	%%
	%% Height	Block/Status
	%%
	%% 1		B2/on_chain      B1_2/not_validated
	%%					  \             /
	%% 0					B1/on_chain
	mark_tip(bcache_test, block_id(B2)),
	?assertEqual(B1_2, get(bcache_test, block_id(B1_2))),
	assert_longest_chain([B1], 0),
	assert_tip(block_id(B2)),
	assert_max_cdiff({2, block_id(B1_2)}),
	assert_is_valid_fork(true, on_chain, B1),
	assert_is_valid_fork(true, on_chain, B2),
	assert_is_valid_fork(true, not_validated, B1_2),

	%% Remove B1_2, causing B2 to now be the tip of the heaviest chain
	%%
	%% Height	Block/Status
	%%
	%% 1		B2/on_chain 
	%%					  \ 
	%% 0					B1/on_chain
	remove(bcache_test, block_id(B1_2)),
	?assertEqual(not_found, get(bcache_test, block_id(B1_2))),
	?assertEqual(B1, get_by_solution_hash(bcache_test, B1#block.hash,
			crypto:strong_rand_bytes(32), 0, 0)),
	assert_longest_chain([B2, B1], 0),
	assert_tip(block_id(B2)),
	assert_max_cdiff({1, block_id(B2)}),
	assert_is_valid_fork(true, on_chain, B1),
	assert_is_valid_fork(true, on_chain, B2),

	%% Height	Block/Status
	%%
	%% 1		B2/on_chain 
	%%					  \ 
	%% 0					B1/on_chain
	prune(bcache_test, 1),
	?assertEqual(B1, get(bcache_test, block_id(B1))),
	?assertEqual(B1, get_by_solution_hash(bcache_test, B1#block.hash,
			crypto:strong_rand_bytes(32), 0, 0)),
	assert_longest_chain([B2, B1], 0),
	assert_tip(block_id(B2)),
	assert_max_cdiff({1, block_id(B2)}),
	assert_is_valid_fork(true, on_chain, B1),
	assert_is_valid_fork(true, on_chain, B2),

	%% Height	Block/Status
	%%
	%% 1		B2/on_chain 
	prune(bcache_test, 0),
	?assertEqual(not_found, get(bcache_test, block_id(B1))),
	?assertEqual(not_found, get_by_solution_hash(bcache_test, B1#block.hash, <<>>, 0, 0)),
	assert_longest_chain([B2], 0),
	assert_tip(block_id(B2)),
	assert_max_cdiff({1, block_id(B2)}),
	assert_is_valid_fork(true, on_chain, B2),

	prune(bcache_test, 0),
	?assertEqual(not_found, get(bcache_test, block_id(B1_2))),
	?assertEqual(not_found, get_by_solution_hash(bcache_test, B1_2#block.hash, <<>>, 0, 0)),
	assert_longest_chain([B2], 0),
	assert_tip(block_id(B2)),
	assert_max_cdiff({1, block_id(B2)}),
	assert_is_valid_fork(true, on_chain, B2),

	%% B1_2->B1 fork is the heaviest, but only B1 is validated. B2_2->B2->B1 is longer but
	%% has a lower cdiff.
	%%
	%% Height	Block/Status
	%%
	%% 2		B2_2/not_validated
	%%               |
	%% 1		B2/on_chain      B1_2/not_validated
	%%					  \             /
	%% 0					B1/on_chain
	new(bcache_test, B1),
	add(bcache_test, B1_2),
	add(bcache_test, B2),
	mark_tip(bcache_test, block_id(B2)),
	add(bcache_test, B2_2 = on_top(random_block(1), B2)),
	?assertMatch({B1_2, [B1], {{not_validated, ExpectedStatus}, _Timestamp}},
			get_earliest_not_validated_from_longest_chain(bcache_test)),
	assert_longest_chain([B1], 0),
	assert_tip(block_id(B2)),
	assert_max_cdiff({2, block_id(B1_2)}),
	assert_is_valid_fork(true, on_chain, B1),
	assert_is_valid_fork(true, on_chain, B2),
	assert_is_valid_fork(true, not_validated, B1_2),
	assert_is_valid_fork(true, not_validated, B2_2),

	%% B2_3->B2_2->B2->B1 is no longer and heavier but only B2->B1 are validated.
	%%
	%% Height	Block/Status
	%%
	%% 3		B2_3/not_validated
	%%               |
	%% 2		B2_2/not_validated
	%%               |
	%% 1		B2/on_chain      B1_2/not_validated
	%%					  \             /
	%% 0					B1/on_chain
	add(bcache_test, B2_3 = on_top(random_block(3), B2_2)),
	?assertMatch({B2_2, [B2], {{not_validated, ExpectedStatus}, _Timestamp}},
			get_earliest_not_validated_from_longest_chain(bcache_test)),
	?assertException(error, invalid_tip, mark_tip(bcache_test, block_id(B2_3))),
	assert_longest_chain([B2, B1], 0),
	assert_tip(block_id(B2)),
	assert_max_cdiff({3, block_id(B2_3)}),
	assert_is_valid_fork(true, on_chain, B1),
	assert_is_valid_fork(true, on_chain, B2),
	assert_is_valid_fork(true, not_validated, B1_2),
	assert_is_valid_fork(true, not_validated, B2_2),
	assert_is_valid_fork(true, not_validated, B2_3),

	%% Now B2_2->B2->B1 are validated.
	%%
	%% Height	Block/Status
	%%
	%% 3		B2_3/not_validated
	%%               |
	%% 2		B2_2/validated
	%%               |
	%% 1		B2/on_chain      B1_2/not_validated
	%%					  \             /
	%% 0					B1/on_chain
	add_validated(bcache_test, B2_2),
	?assertMatch({B2_2, {validated, _}},
			get_block_and_status(bcache_test, B2_2#block.indep_hash)),
	?assertMatch({B2_3, [B2_2, B2], {{not_validated, ExpectedStatus}, _}},
			get_earliest_not_validated_from_longest_chain(bcache_test)),
	assert_longest_chain([B2_2, B2, B1], 1),
	assert_tip(block_id(B2)),
	assert_max_cdiff({3, block_id(B2_3)}),
	assert_is_valid_fork(true, on_chain, B1),
	assert_is_valid_fork(true, on_chain, B2),
	assert_is_valid_fork(true, not_validated, B1_2),
	assert_is_valid_fork(true, validated, B2_2),
	assert_is_valid_fork(true, not_validated, B2_3),

	%% Now the B3->B2->B1 fork is heaviest
	%%
	%% Height	Block/Status
	%%
	%% 3		B2_3/not_validated
	%%               |
	%% 2		B2_2/validated          B3/on_chain
	%%                    \            /
	%% 1                   B2/on_chain      B1_2/not_validated
	%%			                    \           /
	%% 0                             B1/on_chain
	B3 = on_top(random_block(4), B2),
	B3ID = block_id(B3),
	add(bcache_test, B3),
	add_validated(bcache_test, B3),
	mark_tip(bcache_test, B3ID),
	?assertEqual(not_found, get_earliest_not_validated_from_longest_chain(bcache_test)),
	assert_longest_chain([B3, B2, B1], 0),
	assert_tip(block_id(B3)),
	assert_max_cdiff({4, block_id(B3)}),
	assert_is_valid_fork(true, on_chain, B1),
	assert_is_valid_fork(true, on_chain, B2),
	assert_is_valid_fork(true, not_validated, B1_2),
	assert_is_valid_fork(true, validated, B2_2),
	assert_is_valid_fork(true, not_validated, B2_3),
	assert_is_valid_fork(true, on_chain, B3),
	?assertEqual([B2_2], get_siblings(bcache_test, B3)),
	?assertEqual([B3], get_siblings(bcache_test, B2_2)),
	?assertEqual([B2], get_siblings(bcache_test, B1_2)),

	%% B3->B2->B1 fork is still heaviest
	%%
	%% Height	Block/Status
	%%
	%% 3		B2_3/not_validated
	%%               |
	%% 2		B2_2/on_chain        B3/validated
	%%                    \            /
	%% 1                   B2/on_chain      B1_2/not_validated
	%%			                    \           /
	%% 0                             B1/on_chain
	mark_tip(bcache_test, block_id(B2_2)),
	?assertEqual(not_found, get_earliest_not_validated_from_longest_chain(bcache_test)),
	assert_longest_chain([B3, B2, B1], 1),
	assert_tip(block_id(B2_2)),
	assert_max_cdiff({4, block_id(B3)}),
	assert_is_valid_fork(true, on_chain, B1),
	assert_is_valid_fork(true, on_chain, B2),
	assert_is_valid_fork(true, not_validated, B1_2),
	assert_is_valid_fork(true, on_chain, B2_2),
	assert_is_valid_fork(true, not_validated, B2_3),
	assert_is_valid_fork(true, validated, B3),

	%% Height	Block/Status
	%%
	%% 3		B2_3/not_validated   B4/not_validated
	%%               |                     |
	%% 2		B2_2/on_chain        B3/validated
	%%                    \            /
	%% 1                   B2/on_chain      B1_2/not_validated
	%%			                    \           /
	%% 0                             B1/on_chain
	add(bcache_test, B4 = on_top(random_block(5), B3)),
	?assertMatch({B4, [B3, B2], {{not_validated, ExpectedStatus}, _Timestamp}},
			get_earliest_not_validated_from_longest_chain(bcache_test)),
	assert_longest_chain([B3, B2, B1], 1),
	assert_tip(block_id(B2_2)),
	assert_max_cdiff({5, block_id(B4)}),
	assert_is_valid_fork(true, on_chain, B1),
	assert_is_valid_fork(true, on_chain, B2),
	assert_is_valid_fork(true, not_validated, B1_2),
	assert_is_valid_fork(true, on_chain, B2_2),
	assert_is_valid_fork(true, not_validated, B2_3),
	assert_is_valid_fork(true, validated, B3),
	assert_is_valid_fork(true, not_validated, B4),

	%% Height	Block/Status
	%%
	%% 3		B2_3/not_validated   B4/not_validated
	%%               |                     |
	%% 2		B2_2/on_chain        B3/validated
	%%                    \            /
	%% 1                   B2/on_chain
	prune(bcache_test, 1),
	?assertEqual(not_found, get(bcache_test, block_id(B1))),
	?assertEqual(not_found, get_by_solution_hash(bcache_test, B1#block.hash, <<>>, 0, 0)),
	assert_longest_chain([B3, B2], 1),
	assert_tip(block_id(B2_2)),
	assert_max_cdiff({5, block_id(B4)}),
	assert_is_valid_fork(true, on_chain, B2),
	assert_is_valid_fork(true, on_chain, B2_2),
	assert_is_valid_fork(true, not_validated, B2_3),
	assert_is_valid_fork(true, validated, B3),
	assert_is_valid_fork(true, not_validated, B4),
	?assertEqual([], get_siblings(bcache_test, B2_3)),
	?assertEqual([], get_siblings(bcache_test, B4)),

	%% Height	Block/Status
	%%
	%% 3		B2_3/on_chain
	%%               |
	%% 2		B2_2/on_chain
	mark_tip(bcache_test, block_id(B2_3)),
	prune(bcache_test, 1),
	?assertEqual(not_found, get(bcache_test, block_id(B2))),
	?assertEqual(not_found, get_by_solution_hash(bcache_test, B2#block.hash, <<>>, 0, 0)),
	assert_longest_chain([B2_3, B2_2], 0),
	assert_tip(block_id(B2_3)),
	assert_max_cdiff({3, block_id(B2_3)}),
	assert_is_valid_fork(true, on_chain, B2_2),
	assert_is_valid_fork(true, on_chain, B2_3),

	%% Height	Block/Status
	%%
	%% 3		B2_3/on_chain
	%%               |
	%% 2		B2_2/on_chain
	prune(bcache_test, 1),
	?assertEqual(not_found, get(bcache_test, block_id(B3))),
	?assertEqual(not_found, get_by_solution_hash(bcache_test, B3#block.hash, <<>>, 0, 0)),
	assert_longest_chain([B2_3, B2_2], 0),
	assert_tip(block_id(B2_3)),
	assert_max_cdiff({3, block_id(B2_3)}),
	assert_is_valid_fork(true, on_chain, B2_2),
	assert_is_valid_fork(true, on_chain, B2_3),

	%% Height	Block/Status
	%%
	%% 3		B2_3/on_chain
	%%               |
	%% 2		B2_2/on_chain
	prune(bcache_test, 1),
	?assertEqual(not_found, get(bcache_test, block_id(B4))),
	?assertEqual(not_found, get_by_solution_hash(bcache_test, B4#block.hash, <<>>, 0, 0)),
	assert_longest_chain([B2_3, B2_2], 0),
	assert_tip(block_id(B2_3)),
	assert_max_cdiff({3, block_id(B2_3)}),
	assert_is_valid_fork(true, on_chain, B2_2),
	assert_is_valid_fork(true, on_chain, B2_3),

	%% Height	Block/Status
	%%
	%% 3		B2_3/on_chain
	%%               |
	%% 2		B2_2/on_chain
	prune(bcache_test, 1),
	?assertEqual(B2_2, get(bcache_test, block_id(B2_2))),
	?assertEqual(B2_2, get_by_solution_hash(bcache_test, B2_2#block.hash, <<>>, 0, 0)),
	assert_longest_chain([B2_3, B2_2], 0),
	assert_tip(block_id(B2_3)),
	assert_max_cdiff({3, block_id(B2_3)}),
	assert_is_valid_fork(true, on_chain, B2_2),
	assert_is_valid_fork(true, on_chain, B2_3),

	%% Height	Block/Status
	%%
	%% 3		B2_3/on_chain
	%%               |
	%% 2		B2_2/on_chain
	prune(bcache_test, 1),
	?assertEqual(B2_3, get(bcache_test, block_id(B2_3))),
	?assertEqual(B2_3, get_by_solution_hash(bcache_test, B2_3#block.hash, <<>>, 0, 0)),
	assert_longest_chain([B2_3, B2_2], 0),
	assert_tip(block_id(B2_3)),
	assert_max_cdiff({3, block_id(B2_3)}),
	assert_is_valid_fork(true, on_chain, B2_2),
	assert_is_valid_fork(true, on_chain, B2_3),

	%% Height	Block/Status
	%%
	%% 3		B2_3/on_chain
	%%               |
	%% 2		B2_2/on_chain
	remove(bcache_test, block_id(B3)),
	?assertEqual(not_found, get(bcache_test, block_id(B3))),
	?assertEqual(not_found, get_by_solution_hash(bcache_test, B3#block.hash, <<>>, 0, 0)),
	assert_longest_chain([B2_3, B2_2], 0),
	assert_tip(block_id(B2_3)),
	assert_max_cdiff({3, block_id(B2_3)}),
	assert_is_valid_fork(true, on_chain, B2_2),
	assert_is_valid_fork(true, on_chain, B2_3),

	%% Height	Block/Status
	%%
	%% 3		B2_3/on_chain
	%%               |
	%% 2		B2_2/on_chain
	remove(bcache_test, block_id(B3)),
	?assertEqual(not_found, get(bcache_test, block_id(B4))),
	?assertEqual(not_found, get_by_solution_hash(bcache_test, B4#block.hash, <<>>, 0, 0)),
	assert_longest_chain([B2_3, B2_2], 0),
	assert_tip(block_id(B2_3)),
	assert_max_cdiff({3, block_id(B2_3)}),
	assert_is_valid_fork(true, on_chain, B2_2),
	assert_is_valid_fork(true, on_chain, B2_3),

	%% Height	Block/Status
	%%
	%% 1		B12/not_validated    B13/on_chain
	%%					  \              /
	%% 0					B11/on_chain
	new(bcache_test, B11 = random_block(0)),
	add(bcache_test, B12 = on_top(random_block(1), B11)),
	add_validated(bcache_test, B13 = on_top(random_block(1), B11)),
	mark_tip(bcache_test, block_id(B13)),
	%% Although the first block at height 1 was the one added in B12, B13 then
	%% became the tip so we should not reorganize.
	?assertEqual(not_found, get_earliest_not_validated_from_longest_chain(bcache_test)),
	%% The longest chain starts a the max_cdiff block which in this case is B12 since B13
	%% was added second and has the same cdiff. So the longest chain stays as just [B11]
	assert_longest_chain([B11], 0),
	assert_tip(block_id(B13)),
	assert_max_cdiff({1, block_id(B12)}),
	assert_is_valid_fork(true, on_chain, B11),
	assert_is_valid_fork(true, not_validated, B12),
	assert_is_valid_fork(true, on_chain, B13),

	%% Height	Block/Status
	%%
	%% 2							 B14/not_validated
	%%									  |
	%% 1		B12/not_validated    B13/on_chain
	%%					  \              /
	%% 0					B11/on_chain
	add(bcache_test, B14 = on_top(random_block_after_repacking(2), B13)),
	?assertMatch({B14, [B13], {{not_validated, awaiting_nonce_limiter_validation}, _}},
			get_earliest_not_validated_from_longest_chain(bcache_test)),
	assert_longest_chain([B13, B11], 0),
	assert_tip(block_id(B13)),
	assert_max_cdiff({2, block_id(B14)}),
	assert_is_valid_fork(true, on_chain, B11),
	assert_is_valid_fork(true, not_validated, B12),
	assert_is_valid_fork(true, on_chain, B13),
	assert_is_valid_fork(true, not_validated, B14),

	%% Height	Block/Status
	%%
	%% 2							 B14/not_validated
	%%									  |
	%% 1		B12/not_validated    B13/on_chain
	%%					  \              /
	%% 0					B11/on_chain
	mark_nonce_limiter_validated(bcache_test, crypto:strong_rand_bytes(48)),
	mark_nonce_limiter_validated(bcache_test, block_id(B13)),
	?assertMatch({B13, {on_chain, _}},
			get_block_and_status(bcache_test, block_id(B13))),
	?assertMatch({B14, {{not_validated, awaiting_nonce_limiter_validation}, _}},
			get_block_and_status(bcache_test, block_id(B14))),
	assert_longest_chain([B13, B11], 0),
	assert_tip(block_id(B13)),
	assert_max_cdiff({2, block_id(B14)}),
	assert_is_valid_fork(true, on_chain, B11),
	assert_is_valid_fork(true, not_validated, B12),
	assert_is_valid_fork(true, on_chain, B13),
	assert_is_valid_fork(true, not_validated, B14),

	%% Height	Block/Status
	%%
	%% 2							 B14/not_validated
	%%									  |
	%% 1		B12/not_validated    B13/on_chain
	%%					  \              /
	%% 0					B11/on_chain
	?assertMatch({B14, {{not_validated, awaiting_nonce_limiter_validation}, _}},
			get_block_and_status(bcache_test, block_id(B14))),
	?assertMatch({B14, [B13], {{not_validated, awaiting_nonce_limiter_validation}, _}},
			get_earliest_not_validated_from_longest_chain(bcache_test)),
	assert_longest_chain([B13, B11], 0),
	assert_tip(block_id(B13)),
	assert_max_cdiff({2, block_id(B14)}),
	assert_is_valid_fork(true, on_chain, B11),
	assert_is_valid_fork(true, not_validated, B12),
	assert_is_valid_fork(true, on_chain, B13),
	assert_is_valid_fork(true, not_validated, B14),

	%% Height	Block/Status
	%%
	%% 2							 B14/nonce_limiter_validated
	%%									  |
	%% 1		B12/not_validated    B13/on_chain
	%%					  \              /
	%% 0					B11/on_chain
	mark_nonce_limiter_validated(bcache_test, block_id(B14)),
	?assertMatch({B14, {{not_validated, nonce_limiter_validated}, _}},
			get_block_and_status(bcache_test, block_id(B14))),
	?assertMatch({B14, [B13], {{not_validated, nonce_limiter_validated}, _}},
			get_earliest_not_validated_from_longest_chain(bcache_test)),
	%% Longest chain now includes B14 because its status changed to nonce_limiter_validated
	assert_longest_chain([B14, B13, B11], 1),
	assert_tip(block_id(B13)),
	assert_max_cdiff({2, block_id(B14)}),
	assert_is_valid_fork(true, on_chain, B11),
	assert_is_valid_fork(true, not_validated, B12),
	assert_is_valid_fork(true, on_chain, B13),
	assert_is_valid_fork(true, not_validated, B14),

	%% Height	Block/Status
	%%
	%% 3							 B15/not_validated
	%%									  |
	%% 2							 B14/nonce_limiter_validated
	%%									  |
	%% 1		B12/not_validated    B13/on_chain
	%%					  \              /
	%% 0					B11/on_chain
	add(bcache_test, B15 = on_top(random_block_after_repacking(3), B14)),
	?assertMatch({B14, [B13], {{not_validated, nonce_limiter_validated}, _}},
			get_earliest_not_validated_from_longest_chain(bcache_test)),
	assert_longest_chain([B14, B13, B11], 1),
	assert_tip(block_id(B13)),
	assert_max_cdiff({3, block_id(B15)}),
	assert_is_valid_fork(true, on_chain, B11),
	assert_is_valid_fork(true, not_validated, B12),
	assert_is_valid_fork(true, on_chain, B13),
	assert_is_valid_fork(true, not_validated, B14),
	assert_is_valid_fork(true, not_validated, B15),

	%% Height	Block/Status
	%%
	%% 3							 B15/not_validated
	%%									  |
	%% 2							 B14/validated
	%%									  |
	%% 1		B12/not_validated    B13/on_chain
	%%					  \              /
	%% 0					B11/on_chain
	add_validated(bcache_test, B14),
	?assertMatch({B15, [B14, B13], {{not_validated, awaiting_nonce_limiter_validation}, _}},
			get_earliest_not_validated_from_longest_chain(bcache_test)),
	?assertMatch({B14, {validated, _}}, get_block_and_status(bcache_test, block_id(B14))),
	assert_longest_chain([B14, B13, B11], 1),
	assert_tip(block_id(B13)),
	assert_max_cdiff({3, block_id(B15)}),
	assert_is_valid_fork(true, on_chain, B11),
	assert_is_valid_fork(true, not_validated, B12),
	assert_is_valid_fork(true, on_chain, B13),
	assert_is_valid_fork(true, validated, B14),
	assert_is_valid_fork(true, not_validated, B15),

	%% Height	Block/Status
	%%
	%% 3							 B16/not_validated
	%%									  |
	%% 3							 B15/not_validated
	%%									  |
	%% 2							 B14/validated
	%%									  |
	%% 1		B12/not_validated    B13/on_chain
	%%					  \              /
	%% 0					B11/on_chain
	add(bcache_test, B16 = on_top(random_block_after_repacking(4), B15)),
	?assertMatch({B15, [B14, B13], {{not_validated, awaiting_nonce_limiter_validation}, _}},
			get_earliest_not_validated_from_longest_chain(bcache_test)),
	assert_longest_chain([B14, B13, B11], 1),
	assert_tip(block_id(B13)),
	assert_max_cdiff({4, block_id(B16)}),
	assert_is_valid_fork(true, on_chain, B11),
	assert_is_valid_fork(true, not_validated, B12),
	assert_is_valid_fork(true, on_chain, B13),
	assert_is_valid_fork(true, validated, B14),
	assert_is_valid_fork(true, not_validated, B15),
	assert_is_valid_fork(true, not_validated, B16),	

	%% Height	Block/Status
	%%
	%% 3							 B16/nonce_limiter_validated
	%%									  |
	%% 3							 B15/not_validated
	%%									  |
	%% 2							 B14/validated
	%%									  |
	%% 1		B12/not_validated    B13/on_chain
	%%					  \              /
	%% 0					B11/on_chain
	mark_nonce_limiter_validated(bcache_test, block_id(B16)),
	?assertMatch({B15, [B14, B13], {{not_validated, awaiting_nonce_limiter_validation}, _}},
			get_earliest_not_validated_from_longest_chain(bcache_test)),
	?assertMatch({B16, {{not_validated, nonce_limiter_validated}, _}},
			get_block_and_status(bcache_test, block_id(B16))),
	assert_longest_chain([B14, B13, B11], 1),
	assert_tip(block_id(B13)),
	assert_max_cdiff({4, block_id(B16)}),
	assert_is_valid_fork(true, on_chain, B11),
	assert_is_valid_fork(true, not_validated, B12),
	assert_is_valid_fork(true, on_chain, B13),
	assert_is_valid_fork(true, validated, B14),
	assert_is_valid_fork(true, not_validated, B15),
	assert_is_valid_fork(true, not_validated, B16),		

	%% Height	Block/Status
	%%
	%% 3							 B16/nonce_limiter_validated
	%%									  |
	%% 3							 B15/not_validated
	%%									  |
	%% 2							 B14/on_chain
	%%									  |
	%% 1		B12/not_validated    B13/on_chain
	%%					  \              /
	%% 0					B11/on_chain
	mark_tip(bcache_test, block_id(B14)),
	?assertMatch({B14, {on_chain, _}}, get_block_and_status(bcache_test, block_id(B14))),
	?assertMatch({B15, [B14], {{not_validated, awaiting_nonce_limiter_validation}, _}},
			get_earliest_not_validated_from_longest_chain(bcache_test)),
	assert_longest_chain([B14, B13, B11], 0),
	assert_tip(block_id(B14)),
	assert_max_cdiff({4, block_id(B16)}),
	assert_is_valid_fork(true, on_chain, B11),
	assert_is_valid_fork(true, not_validated, B12),
	assert_is_valid_fork(true, on_chain, B13),
	assert_is_valid_fork(true, on_chain, B14),
	assert_is_valid_fork(true, not_validated, B15),
	assert_is_valid_fork(true, not_validated, B16),		

	ets:delete(bcache_test).

assert_longest_chain(Chain, NotOnChainCount) ->
	ExpectedPairs =  [{B#block.indep_hash, []} || B <- Chain],
	?assertEqual({ExpectedPairs, NotOnChainCount}, get_longest_chain_cache(bcache_test)).

assert_max_cdiff(ExpectedMaxCDiff) ->
	[{_, MaxCDiff}] = ets:lookup(bcache_test, max_cdiff),
	?assertEqual(ExpectedMaxCDiff, MaxCDiff).

assert_is_valid_fork(ExpectedFork, ExpectedStatus, B) ->
	[{_, {_, Status, _, _}}] = ets:lookup(bcache_test, {block, block_id(B)}),
	case ExpectedStatus of
		not_validated ->
			?assertMatch({not_validated, _}, Status);
		_ ->
			?assertEqual(ExpectedStatus, Status)
	end,
	?assertEqual(ExpectedFork, is_valid_fork(bcache_test, B, Status)).	

assert_tip(ExpectedTip) ->
	[{_, Tip}] = ets:lookup(bcache_test, tip),
	?assertEqual(ExpectedTip,Tip).

random_block(CDiff) ->
	#block{ indep_hash = crypto:strong_rand_bytes(48), height = 0, cumulative_diff = CDiff,
			hash = crypto:strong_rand_bytes(32) }.

random_block_after_repacking(CDiff) ->
	#block{ indep_hash = crypto:strong_rand_bytes(48), height = 0, cumulative_diff = CDiff,
			hash = crypto:strong_rand_bytes(32) }.

block_id(#block{ indep_hash = H }) ->
	H.

on_top(B, PrevB) ->
	B#block{ previous_block = PrevB#block.indep_hash, height = PrevB#block.height + 1,
			previous_cumulative_diff = PrevB#block.cumulative_diff }.

]]>
</file>

<file path="/Users/peng.li/workspace/arweave/apps/arweave/src/ar_block_index.erl">
<![CDATA[
-module(ar_block_index).

-export([init/1, update/2, member/1, get_list/1, get_list_by_hash/1, get_element_by_height/1,
		get_block_bounds/1, get_intersection/2, get_intersection/1, get_range/2, get_last/0]).

%%%===================================================================
%%% Public interface.
%%%===================================================================

%% @doc Store the given block index in ETS.
init(BI) ->
	init(lists:reverse(BI), 0).

%% @doc Insert the new block index elements from BI and remove the N orphaned ones.
update([], 0) ->
	ok;
update(BI, 0) ->
	{_WeaveSize, Height, _H, _TXRoot} = ets:last(block_index),
	update2(BI, Height + 1);
update(BI, N) ->
	ets:delete(block_index, ets:last(block_index)),
	update(BI, N - 1).

%% @doc Return true if the given block hash is found in the index.
member(H) ->
	member(H, ets:last(block_index)).

%% @doc Return the list of {H, WeaveSize, TXRoot} triplets up to the given Height (including)
%% sorted from latest to earliest.
get_list(Height) ->
	get_list([], ets:first(block_index), -1, Height).

%% @doc Return the list of {H, WeaveSize, TXRoot} triplets up to the block with the given
%% hash H (including) sorted from latest to earliest.
get_list_by_hash(H) ->
	get_list_by_hash([], ets:first(block_index), -1, H).

%% @doc Return the {H, WeaveSize, TXRoot} triplet for the given Height or not_found.
get_element_by_height(Height) ->
	case catch ets:slot(block_index, Height) of
		{'EXIT', _} ->
			not_found;
		'$end_of_table' ->
			not_found;
		[{{WeaveSize, Height, H, TXRoot}}] ->
			{H, WeaveSize, TXRoot}
	end.

%% @doc Return {BlockStartOffset, BlockEndOffset, TXRoot} where Offset >= BlockStartOffset,
%% Offset < BlockEndOffset.
get_block_bounds(Offset) ->
	{WeaveSize, Height, _H, TXRoot} = Key = ets:next(block_index, {Offset, n, n, n}),
	case Height of
		0 ->
			{0, WeaveSize, TXRoot};
		_ ->
			{PrevWeaveSize, _, _, _} = ets:prev(block_index, Key),
			{PrevWeaveSize, WeaveSize, TXRoot}
	end.

%% @doc Return {Height, {H, WeaveSize, TXRoot}} with the triplet present in both
%% the cached block index and the given BI or no_intersection.
get_intersection(Height, _BI) when Height < 0 ->
	no_intersection;
get_intersection(_Height, []) ->
	no_intersection;
get_intersection(Height, BI) ->
	ReverseBI = lists:reverse(BI),
	[{H, _, _} = Elem | ReverseBI2] = ReverseBI,
	case catch ets:slot(block_index, Height) of
		[{{_, Height, H, _} = Entry}] ->
			get_intersection(Height + 1, Elem, ReverseBI2, ets:next(block_index, Entry));
		_ ->
			no_intersection
	end.

%% @doc Return the {H, WeaveSize, TXRoot} triplet present in both
%% the cached block index and the given BI or no_intersection.
get_intersection([]) ->
	no_intersection;
get_intersection(BI) ->
	{H, WeaveSize, _TXRoot} = lists:last(BI),
	get_intersection2({H, WeaveSize}, tl(lists:reverse(BI)),
			ets:next(block_index, {WeaveSize - 1, n, n, n})).

%% @doc Return the list of {H, WeaveSize, TXRoot} for blocks with Height >= Start, =< End,
%% sorted from the largest height to the smallest.
get_range(Start, End) when Start > End ->
	[];
get_range(Start, End) ->
	case catch ets:slot(block_index, Start) of
		[{{WeaveSize, _Height, H, TXRoot} = Entry}] ->
			lists:reverse([{H, WeaveSize, TXRoot}
				| get_range2(Start + 1, End, ets:next(block_index, Entry))]);
		_ ->
			{error, invalid_start}
	end.

%% @doc Return the last element in the block index.
get_last() ->
	ets:last(block_index).

%%%===================================================================
%%% Private functions.
%%%===================================================================

init([], _Height) ->
	ok;
init([{H, WeaveSize, TXRoot} | BI], Height) ->
	ets:insert(block_index, {{WeaveSize, Height, H, TXRoot}}),
	init(BI, Height + 1).

update2([], _Height) ->
	ok;
update2([{H, WeaveSize, TXRoot} | BI], Height) ->
	ets:insert(block_index, {{WeaveSize, Height, H, TXRoot}}),
	update2(BI, Height + 1).

member(H, {_, _, H, _}) ->
	true;
member(_H, '$end_of_table') ->
	false;
member(H, Key) ->
	member(H, ets:prev(block_index, Key)).

get_list(BI, '$end_of_table', _Height, _MaxHeight) ->
	BI;
get_list(BI, _Elem, Height, MaxHeight) when Height >= MaxHeight ->
	BI;
get_list(BI, {WeaveSize, NextHeight, H, TXRoot} = Key, Height, MaxHeight)
		when NextHeight == Height + 1 ->
	get_list([{H, WeaveSize, TXRoot} | BI], ets:next(block_index, Key), Height + 1, MaxHeight);
get_list(_BI, _Key, _Height, MaxHeight) ->
	%% An extremely unlikely race condition should have occured where some blocks were
	%% orphaned right after we passed some of them here, and new blocks have been added
	%% right before we reached the end of the table.
	get_list(MaxHeight).

get_list_by_hash(BI, '$end_of_table', _Height, _H) ->
	BI;
get_list_by_hash(BI, {WeaveSize, NextHeight, H, TXRoot}, Height, H)
		when NextHeight == Height + 1 ->
	[{H, WeaveSize, TXRoot} | BI];
get_list_by_hash(BI, {WeaveSize, NextHeight, H, TXRoot} = Key, Height, H2)
		when NextHeight == Height + 1 ->
	get_list_by_hash([{H, WeaveSize, TXRoot} | BI], ets:next(block_index, Key), Height + 1,
			H2);
get_list_by_hash(_BI, _Key, _Height, H) ->
	%% An extremely unlikely race condition should have occured where some blocks were
	%% orphaned right after we passed some of them here, and new blocks have been added
	%% right before we reached the end of the table.
	get_list_by_hash(H).

get_intersection(Height, Entry, _ReverseBI, '$end_of_table') ->
	{Height - 1, Entry};
get_intersection(Height, Entry, [], _Entry) ->
	{Height - 1, Entry};
get_intersection(Height, _Entry, [{H, _, _} = Elem | ReverseBI], {_, Height, H, _} = Entry) ->
	get_intersection(Height + 1, Elem, ReverseBI, ets:next(block_index, Entry));
get_intersection(Height, Entry, _ReverseBI, _TableEntry) ->
	{Height - 1, Entry}.

get_intersection2(_, _, '$end_of_table') ->
	no_intersection;
get_intersection2({_, WeaveSize}, _, {WeaveSize2, _, _, _}) when WeaveSize2 > WeaveSize ->
	no_intersection;
get_intersection2({H, WeaveSize}, BI, {WeaveSize, _, H, TXRoot} = Elem) ->
	get_intersection3(ets:next(block_index, Elem), BI, {H, WeaveSize, TXRoot});
get_intersection2({H, WeaveSize}, BI, {WeaveSize, _, _, _} = Elem) ->
	get_intersection2({H, WeaveSize}, BI, ets:next(block_index, Elem)).

get_intersection3({WeaveSize, _, H, TXRoot} = Key, [{H, WeaveSize, TXRoot} | BI], _Elem) ->
	get_intersection3(ets:next(block_index, Key), BI, {H, WeaveSize, TXRoot});
get_intersection3(_, _, {H, WeaveSize, TXRoot}) ->
	{H, WeaveSize, TXRoot}.

get_range2(Start, End, _Elem) when Start > End ->
	[];
get_range2(_Start, _End, '$end_of_table') ->
	[];
get_range2(Start, End, {WeaveSize, _Height, H, TXRoot} = Elem) ->
	[{H, WeaveSize, TXRoot} | get_range2(Start + 1, End, ets:next(block_index, Elem))].

]]>
</file>

